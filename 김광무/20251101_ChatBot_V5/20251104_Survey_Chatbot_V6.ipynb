{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mykeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•„ë˜ ë§í¬ë¥¼ ë³µì‚¬í•˜ì—¬ ì›¹ ë¸Œë¼ìš°ì €ì— ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.\n",
      "https://accounts.google.com/o/oauth2/auth?client_id=35726703810-4v13dfqmilhgv6shlc3cv9i3ktuh73j1.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "Authentication successful.\n",
      "mykey => set HUGGINGFACEHUB_API_TOKEN is done\n",
      "mykey => set OPENAI_API_KEY is done\n",
      "mykey => set LANGCHAIN_API_KEY is done\n",
      "mykey => set LANGCHAIN_HUB_API_KEY is done\n",
      "mykey => set GOOGLE_API_KEY is done\n",
      "mykey => set GOOGLE_CSE_ID is done\n",
      "mykey => set UPSTAGE_API_KEY is done\n",
      "mykey => set COHERE_API_KEY is done\n",
      "mykey => set JINA_API_KEY is done\n",
      "mykey => set ANTHROPIC_API_KEY is done\n",
      "mykey => set DEEPL_API_KEY is done\n",
      "mykey => set TAVILY_API_KEY is done\n",
      "mykey => set TOGETHER_API_KEY is done\n"
     ]
    }
   ],
   "source": [
    "mykeys.setOsEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time \n",
    "import ast\n",
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import openai\n",
    "import random\n",
    "import gradio as gr\n",
    "from urllib.parse import urlparse, quote # (â˜…â˜…â˜… 'quote' ì¶”ê°€ â˜…â˜…â˜…)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2/4: Global Variables (ì „ì—­ ë³€ìˆ˜ ë° ì„¤ì •) ---\n",
    "RESTAURANT_DB_FILE = \"restaurant_summaries_output_ALL.csv\"\n",
    "MENU_DB_FILE = \"20251017_TOTAL_MENU.csv\"\n",
    "DB_PERSISTENT_PATH = \"./restaurant_db\"\n",
    "\n",
    "# (â˜…â˜…â˜… 1. í”„ë¡œí•„ DBìš© ì „ì—­ ë³€ìˆ˜ ì¶”ê°€ â˜…â˜…â˜…)\n",
    "PROFILE_DB_FILE = \"user_profiles_for_hybrid_search.csv\" # (ìƒˆ íŒŒì¼)\n",
    "RESTAURANT_COLLECTION_NAME = \"restaurants\" # (ì´ë¦„ ë³€ê²½)\n",
    "PROFILE_COLLECTION_NAME = \"mock_profiles\" # (ìƒˆ ì»¬ë ‰ì…˜ ì´ë¦„)\n",
    "\n",
    "# (Gradioê°€ ì‚¬ìš©í•  ì „ì—­ DB ë³€ìˆ˜)\n",
    "df_restaurants = None\n",
    "df_menus = None\n",
    "collection = None # ('restaurants' ì»¬ë ‰ì…˜ ì „ìš©)\n",
    "profile_collection = None # (â˜…â˜…â˜… 2. í”„ë¡œí•„ ì»¬ë ‰ì…˜ ì „ì—­ ë³€ìˆ˜ ì¶”ê°€ â˜…â˜…â˜…)\n",
    "menu_groups = None # (ë©”ë‰´ ê·¸ë£¹ ì „ì—­ ë³€ìˆ˜)\n",
    "\n",
    "# (500ëª… í‰ê°€ ë°ì´í„° ì €ì¥ìš© ì „ì—­ ë³€ìˆ˜ 2ê°œ)\n",
    "df_all_user_ratings = None \n",
    "df_restaurant_ratings_summary = None \n",
    "\n",
    "# (DB ì¬êµ¬ì¶• ì„¤ì •)\n",
    "CLEAR_DB_AND_REBUILD = False\n",
    "\n",
    "# (â˜…â˜…â˜… 3. ìœ ì‚¬ ì‚¬ìš©ì ì¶”ì²œìš© ì „ì—­ ë³€ìˆ˜ ì¶”ê°€ â˜…â˜…â˜…)\n",
    "sentence_embedder = None # (SentenceTransformer ëª¨ë¸)\n",
    "\n",
    "# (LLM API ì„¤ì •)\n",
    "GPT_API_NAME = \"gpt-4.1-mini\" \n",
    "try:\n",
    "  client = openai.OpenAI() \n",
    "  if not client.api_key:\n",
    "    client.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not client.api_key:\n",
    "      raise openai.OpenAIError(\"OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "  print(f\"API í‚¤ ë¡œë“œ ì˜¤ë¥˜: {e}\")\n",
    "  exit() # (API í‚¤ ì—†ìœ¼ë©´ ì¢…ë£Œ)\n",
    "\n",
    "# --- 2. (ë©”ì¸) ì±—ë´‡ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ---\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "ë‹¹ì‹ ì€ ë§¤ìš° ì¹œì ˆí•˜ê³  ì§€ëŠ¥ì ì¸ í•œêµ­ ì—¬í–‰ ë„ìš°ë¯¸ ì±—ë´‡ì…ë‹ˆë‹¤.\n",
    "ë‹¹ì‹ ì˜ ìœ ì¼í•œ ì„ë¬´ëŠ” ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ë©°, 13ê°€ì§€ í•„ìˆ˜ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì—¬ JSON í”„ë¡œí•„ì„ ì™„ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "[ìˆ˜ì§‘í•´ì•¼ í•  13ê°œ í•­ëª© ìŠ¤í‚¤ë§ˆ]\n",
    "0.  name: (ì‚¬ìš©ìì˜ ì´ë¦„, ì˜ˆ: \"Lucas Fernandez\", \"Soojin Kim\")\n",
    "1.  age: (ì˜ˆ: \"10ëŒ€\", \"20ëŒ€\", \"30ëŒ€\"...)\n",
    "2.  gender: (ì˜ˆ: \"ë‚¨\", \"ì—¬\", \"ê¸°íƒ€\")\n",
    "3.  nationality: (ì˜ˆ: \"ë¯¸êµ­\", \"ì¼ë³¸\", \"ì¤‘êµ­\")\n",
    "4.  travel_type: (ì˜ˆ: \"ê°€ì¡±\", \"í˜¼ì\", \"ì¹œêµ¬\", \"ì—°ì¸\")\n",
    "5.  party_size: (ì˜ˆ: 1, 2, 4...)\n",
    "6.  can_wait: (ì›¨ì´íŒ… ê°€ëŠ¥ ì—¬ë¶€, ì˜ˆ: \"O\", \"X\")\n",
    "7.  budget: (ì˜ˆì‚° ìˆ˜ì¤€, ì˜ˆ: \"ì €\", \"ì¤‘\", \"ê³ \")\n",
    "8.  spicy_ok: (ë§¤ìš´ ìŒì‹ ê°€ëŠ¥ ì—¬ë¶€, ì˜ˆ: \"O\", \"X\")\n",
    "9.  is_vegetarian: (ì±„ì‹ ì—¬ë¶€, ì˜ˆ: \"O\", \"X\")\n",
    "10. avoid_ingredients: (ì ˆëŒ€ ë¶ˆê°€ ì‹ì¬ë£Œ, ì˜ˆ: \"ë¼ì§€ê³ ê¸°\", \"ê²¬ê³¼ë¥˜\", \"ì—†ìŒ\")\n",
    "11. like_ingredients: (ì¢‹ì•„í•˜ëŠ” ì‹ì¬ë£Œ, ì˜ˆ: \"ë‹­ê³ ê¸°\", \"í•´ì‚°ë¬¼\", \"ì•¼ì±„\")\n",
    "12. food_category: (ì„ í˜¸ ìŒì‹ ë¶„ë¥˜, ì˜ˆ: \"í•œì‹\", \"ì¼ì‹\", \"ë””ì €íŠ¸\", \"ìƒê´€ì—†ìŒ\")\n",
    "\n",
    "[ëŒ€í™” ê·œì¹™]\n",
    "1.  ëŒ€í™”ëŠ” ë‹¹ì‹ ì´ ë¨¼ì € ì‹œì‘í•©ë‹ˆë‹¤. í™˜ì˜ ì¸ì‚¬ì™€ í•¨ê»˜ ì²« ì§ˆë¬¸(ì˜ˆ: ì„±í•¨)ì„ í•˜ì„¸ìš”.\n",
    "2.  í•­ìƒ í•œ ë²ˆì— í•˜ë‚˜ì”©ë§Œ ì§ˆë¬¸í•˜ì„¸ìš”.\n",
    "3.  ì‚¬ìš©ìì˜ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ [í˜„ì¬ í”„ë¡œí•„]ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
    "4.  ì—…ë°ì´íŠ¸ëœ í”„ë¡œí•„ì„ í™•ì¸í•˜ê³ , ì•„ì§ 'null'ì´ê±°ë‚˜ ìˆ˜ì§‘ë˜ì§€ ì•Šì€ í•­ëª© ì¤‘ í•˜ë‚˜ë¥¼ ê³¨ë¼ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ìŒ ì§ˆë¬¸ì„ í•©ë‹ˆë‹¤.\n",
    "5.  ëª¨ë“  13ê°œ í•­ëª©ì´ ìˆ˜ì§‘ë˜ë©´, \"ì„¤ë¬¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ê°ì‚¬í•©ë‹ˆë‹¤.\"ë¼ëŠ” ë©”ì‹œì§€ë¥¼ ë³´ë‚´ê³  ë” ì´ìƒ ì§ˆë¬¸í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "6.  ë§¤ìš° ì¹œì ˆí•˜ê³  ê³µê°í•˜ëŠ” í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.\n",
    "\n",
    "[í•„ìˆ˜ ì¶œë ¥ í¬ë§·]\n",
    "ë‹¹ì‹ ì€ *ë°˜ë“œì‹œ* ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "{\n",
    "  \"updated_profile\": {\n",
    "    \"name\": \"Lucas Fernandez\", \n",
    "    \"age\": \"20ëŒ€\",\n",
    "    \"gender\": \"ë‚¨\",\n",
    "    \"nationality\": null,\n",
    "    // ... (13ê°œ í•­ëª© ëª¨ë‘ í¬í•¨) ...\n",
    "  },\n",
    "  \"bot_response\": \"Lucasë‹˜ì´ì‹œêµ°ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤. í˜¹ì‹œ ì—°ë ¹ëŒ€ê°€ ì–´ë–»ê²Œ ë˜ì‹œë‚˜ìš”?\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# (13ê°œ í•­ëª© í…œí”Œë¦¿)\n",
    "PROFILE_TEMPLATE = {\n",
    "  \"name\": None, \"age\": None, \"gender\": None, \"nationality\": None, \n",
    "  \"travel_type\": None, \"party_size\": None, \"can_wait\": None, \n",
    "  \"budget\": None, \"spicy_ok\": None, \"is_vegetarian\": None, \n",
    "  \"avoid_ingredients\": None, \"like_ingredients\": None, \"food_category\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3/4: í—¬í¼ í•¨ìˆ˜ ì •ì˜ (ì´ 9ê°œ) ---\n",
    "\n",
    "# --- (í•¨ìˆ˜ 1/9) ---\n",
    "def load_app_data(store_path, menu_path):\n",
    "  \"\"\"\n",
    "  ì•± ì‹¤í–‰ì— í•„ìš”í•œ ëª¨ë“  CSV íŒŒì¼ì„ ë¡œë“œí•˜ì—¬\n",
    "  2ê°œì˜ ì „ì—­ DataFrameì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "  \"\"\"\n",
    "  global df_restaurants, df_menus, menu_groups\n",
    "  \n",
    "  try:\n",
    "    # 1. ê°€ê²Œ DB (ì†Œê°œ, ì£¼ì†Œ ë“±) ë¡œë“œ\n",
    "    print(f\"'{store_path}'ì—ì„œ ê°€ê²Œ DB ë¡œë“œ ì¤‘...\")\n",
    "    df_restaurants = pd.read_csv(store_path)\n",
    "    df_restaurants['id'] = df_restaurants['id'].astype(str)\n",
    "    df_restaurants = df_restaurants.set_index('id') # (idë¡œ ê²€ìƒ‰í•˜ê¸° ì‰½ê²Œ ì¸ë±ìŠ¤ ì„¤ì •)\n",
    "    print(f\"ê°€ê²Œ DB {len(df_restaurants)}ê°œ ë¡œë“œ ì™„ë£Œ.\")\n",
    "    \n",
    "    # 2. ë©”ë‰´ DB (ë©”ë‰´, ê°€ê²©) ë¡œë“œ\n",
    "    print(f\"'{menu_path}'ì—ì„œ ë©”ë‰´ DB ë¡œë“œ ì¤‘...\")\n",
    "    df_menus = pd.read_csv(menu_path)\n",
    "    df_menus['ì‹ë‹¹ID'] = df_menus['ì‹ë‹¹ID'].astype(str)\n",
    "    menu_groups = df_menus.groupby('ì‹ë‹¹ID') # (ì „ì—­ ë³€ìˆ˜ë¡œ ê·¸ë£¹í™”)\n",
    "    print(f\"ë©”ë‰´ DB {len(df_menus)}ê°œ ë¡œë“œ ì™„ë£Œ (ê·¸ë£¹í™” ì™„ë£Œ).\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "  except FileNotFoundError as e:\n",
    "    print(f\"[ì˜¤ë¥˜] í•„ìˆ˜ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    return False\n",
    "  except Exception as e:\n",
    "    print(f\"[ì˜¤ë¥˜] ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (í•¨ìˆ˜ 2/9) ---\n",
    "def load_and_prepare_data(csv_path):\n",
    "  \"\"\"\n",
    "  restaurant_summaries_output...csv íŒŒì¼ì„ ë¡œë“œí•˜ê³ \n",
    "  'ë©”íƒ€ë°ì´í„°' ì»¬ëŸ¼ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "  (DB ì‹ ê·œ êµ¬ì¶• ì‹œì—ë§Œ ì‚¬ìš©ë¨)\n",
    "  \"\"\"\n",
    "  print(f\"'{csv_path}' íŒŒì¼ ë¡œë“œ ì¤‘...\")\n",
    "  try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "  except FileNotFoundError:\n",
    "    print(f\"[ì˜¤ë¥˜] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}\")\n",
    "    return None\n",
    "\n",
    "  def safe_convert_to_dict(x):\n",
    "    if pd.isna(x) or x == '{}' or x == '':\n",
    "      return {}\n",
    "    try:\n",
    "      return ast.literal_eval(x)\n",
    "    except Exception as e:\n",
    "      print(f\"ë©”íƒ€ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {e} \\në°ì´í„°: {x}\")\n",
    "      return {\"error\": \"parsing_failed\"}\n",
    "\n",
    "  print(\"ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ ì¤‘...\")\n",
    "  df['ë©”íƒ€ë°ì´í„°'] = df['ë©”íƒ€ë°ì´í„°'].apply(safe_convert_to_dict)\n",
    "  df['RAGí…ìŠ¤íŠ¸'] = df['RAGí…ìŠ¤íŠ¸'].fillna('')\n",
    "  \n",
    "  print(f\"ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(df)}ê°œ\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (í•¨ìˆ˜ 3/9) ---\n",
    "def build_vector_db(store_csv_path, profile_csv_path, clear_db=False):\n",
    "  \"\"\"\n",
    "  (â˜…â˜…â˜… ìˆ˜ì •ë¨ â˜…â˜…â˜…)\n",
    "  ë ˆìŠ¤í† ë‘ DataFrameê³¼ í”„ë¡œí•„ DataFrameì„ ë°›ì•„\n",
    "  ChromaDBë¥¼ êµ¬ì¶•í•˜ê±°ë‚˜ ë¡œë“œí•©ë‹ˆë‹¤. (2ê°œ ì»¬ë ‰ì…˜)\n",
    "  \"\"\"\n",
    "  global collection, profile_collection, sentence_embedder # (ì „ì—­ ë³€ìˆ˜ 3ê°œ í• ë‹¹)\n",
    "  \n",
    "  print(\"\\n--- 2ë‹¨ê³„: VectorDB êµ¬ì¶•/ë¡œë“œ ì‹œì‘ ---\")\n",
    "  \n",
    "  model_name = \"distiluse-base-multilingual-cased-v1\"\n",
    "  sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=model_name\n",
    "  )\n",
    "  \n",
    "  # (â˜…â˜…â˜… SentenceTransformer ëª¨ë¸ì„ ì „ì—­ ë³€ìˆ˜ì— ì €ì¥ â˜…â˜…â˜…)\n",
    "  sentence_embedder = sentence_transformer_ef._model\n",
    "  print(f\"  > SentenceTransformer ëª¨ë¸ ('{model_name}')ì„ ì „ì—­ 'sentence_embedder'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "  \n",
    "  print(f\"'{DB_PERSISTENT_PATH}' ê²½ë¡œì—ì„œ Persistent DB í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...\")\n",
    "  client = chromadb.PersistentClient(path=DB_PERSISTENT_PATH)\n",
    "\n",
    "  if clear_db:\n",
    "    print(f\"[ê²½ê³ ] CLEAR_DB_AND_REBUILD=True. ì»¬ë ‰ì…˜ 2ê°œ(restaurants, mock_profiles)ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.\")\n",
    "    try:\n",
    "      client.delete_collection(name=RESTAURANT_COLLECTION_NAME)\n",
    "      print(f\"  > '{RESTAURANT_COLLECTION_NAME}' ì‚­ì œ ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "      print(f\"  > '{RESTAURANT_COLLECTION_NAME}' ì‚­ì œ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}\")\n",
    "    try:\n",
    "      client.delete_collection(name=PROFILE_COLLECTION_NAME)\n",
    "      print(f\"  > '{PROFILE_COLLECTION_NAME}' ì‚­ì œ ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "      print(f\"  > '{PROFILE_COLLECTION_NAME}' ì‚­ì œ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}\")\n",
    "\n",
    "  # --- 1. ë ˆìŠ¤í† ë‘ ì»¬ë ‰ì…˜ ë¡œë“œ ---\n",
    "  try:\n",
    "    print(f\"\\n[1/2] ê¸°ì¡´ '{RESTAURANT_COLLECTION_NAME}' ì»¬ë ‰ì…˜ ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤...\")\n",
    "    collection = client.get_collection(\n",
    "      name=RESTAURANT_COLLECTION_NAME,\n",
    "      embedding_function=sentence_transformer_ef\n",
    "    )\n",
    "    print(f\"  > 'restaurants' ë¡œë“œ ì™„ë£Œ: {collection.count()}ê°œ\")\n",
    "  except Exception as e:\n",
    "    print(f\"  > 'restaurants' ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì´ìœ : {e})\")\n",
    "    print(\"  > ìƒˆ 'restaurants' ì»¬ë ‰ì…˜ì„ ìƒì„±í•˜ê³  ë°ì´í„° ì ì¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    df_for_embedding = load_and_prepare_data(store_csv_path)\n",
    "    if df_for_embedding is None:\n",
    "      print(\"[ì˜¤ë¥˜] 'restaurants' DB ì ì¬ë¥¼ ìœ„í•œ ì›ë³¸ CSV ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "      return False\n",
    "\n",
    "    try:\n",
    "      collection = client.create_collection(\n",
    "        name=RESTAURANT_COLLECTION_NAME,\n",
    "        embedding_function=sentence_transformer_ef\n",
    "      )\n",
    "    except Exception as e:\n",
    "      print(f\"[ì˜¤ë¥˜] 'restaurants' ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "      return False\n",
    "\n",
    "    documents_list = df_for_embedding['RAGí…ìŠ¤íŠ¸'].tolist()\n",
    "    metadatas_list = df_for_embedding['ë©”íƒ€ë°ì´í„°'].tolist() \n",
    "    ids_list = df_for_embedding['id'].astype(str).tolist()\n",
    "\n",
    "    print(\"  > 'restaurants' ë©”íƒ€ë°ì´í„° ë³€í™˜ ì¤‘...\")\n",
    "    processed_metadatas = []\n",
    "    for metadata_dict in metadatas_list:\n",
    "      processed_meta_item = {}\n",
    "      for key, value in metadata_dict.items():\n",
    "        if value is None:\n",
    "          processed_meta_item[key] = \"\" \n",
    "        elif isinstance(value, list):\n",
    "          processed_meta_item[key] = \",\".join(map(str, value))\n",
    "        else:\n",
    "          processed_meta_item[key] = value\n",
    "      processed_metadatas.append(processed_meta_item)\n",
    "\n",
    "    print(f\"  > 'restaurants' DBì— {len(ids_list)}ê°œ ì ì¬ ì¤‘ (ë°°ì¹˜)...\")\n",
    "    BATCH_SIZE = 5000\n",
    "    for i in range(0, len(ids_list), BATCH_SIZE):\n",
    "      end_i = min(i + BATCH_SIZE, len(ids_list))\n",
    "      collection.add(\n",
    "        documents=documents_list[i:end_i],\n",
    "        metadatas=processed_metadatas[i:end_i],\n",
    "        ids=ids_list[i:end_i]\n",
    "      )\n",
    "    print(f\"  > 'restaurants' ì‹ ê·œ êµ¬ì¶• ì™„ë£Œ: {collection.count()}ê°œ\")\n",
    "\n",
    "  # --- 2. í”„ë¡œí•„ ì»¬ë ‰ì…˜ ë¡œë“œ ---\n",
    "  try:\n",
    "    print(f\"\\n[2/2] ê¸°ì¡´ '{PROFILE_COLLECTION_NAME}' ì»¬ë ‰ì…˜ ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤...\")\n",
    "    profile_collection = client.get_collection(\n",
    "      name=PROFILE_COLLECTION_NAME,\n",
    "      embedding_function=sentence_transformer_ef\n",
    "    )\n",
    "    print(f\"  > 'mock_profiles' ë¡œë“œ ì™„ë£Œ: {profile_collection.count()}ê°œ\")\n",
    "  except Exception as e:\n",
    "    print(f\"  > 'mock_profiles' ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì´ìœ : {e})\")\n",
    "    print(\"  > ìƒˆ 'mock_profiles' ì»¬ë ‰ì…˜ì„ ìƒì„±í•˜ê³  ë°ì´í„° ì ì¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    try:\n",
    "      # (í”„ë¡œí•„ DB íŒŒì¼ ë¡œë“œ)\n",
    "      df_profiles = pd.read_csv(profile_csv_path)\n",
    "      df_profiles = df_profiles.dropna(subset=['rag_query_text', 'user_id'])\n",
    "      print(f\"  > '{profile_csv_path}' íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(df_profiles)}ê°œ í”„ë¡œí•„\")\n",
    "    except FileNotFoundError:\n",
    "      print(f\"[ì˜¤ë¥˜] '{profile_csv_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "      return False\n",
    "    except Exception as e:\n",
    "      print(f\"[ì˜¤ë¥˜] í”„ë¡œí•„ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "      return False\n",
    "\n",
    "    try:\n",
    "      profile_collection = client.create_collection(\n",
    "        name=PROFILE_COLLECTION_NAME,\n",
    "        embedding_function=sentence_transformer_ef\n",
    "      )\n",
    "    except Exception as e:\n",
    "      print(f\"[ì˜¤ë¥˜] 'mock_profiles' ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "      return False\n",
    "\n",
    "    # (user_idë¥¼ strë¡œ, rag_query_textë¥¼ documentë¡œ)\n",
    "    profile_docs = df_profiles['rag_query_text'].tolist()\n",
    "    profile_ids = df_profiles['user_id'].astype(str).tolist()\n",
    "    # (ë©”íƒ€ë°ì´í„°ì— user_idë¥¼ ì €ì¥í•˜ì—¬ ì¿¼ë¦¬ ì‹œ user_idë¥¼ ë°˜í™˜ë°›ìŒ)\n",
    "    profile_metas = [{'user_id': uid} for uid in profile_ids]\n",
    "\n",
    "    print(f\"  > 'mock_profiles' DBì— {len(profile_ids)}ê°œ ì ì¬ ì¤‘...\")\n",
    "    # (í”„ë¡œí•„ì€ ì–‘ì´ ì ìœ¼ë¯€ë¡œ ë°°ì¹˜ ì²˜ë¦¬ ì•ˆ í•¨)\n",
    "    profile_collection.add(\n",
    "      documents=profile_docs,\n",
    "      metadatas=profile_metas,\n",
    "      ids=profile_ids\n",
    "    )\n",
    "    print(f\"  > 'mock_profiles' ì‹ ê·œ êµ¬ì¶• ì™„ë£Œ: {profile_collection.count()}ê°œ\")\n",
    "\n",
    "  print(f\"--- 2ë‹¨ê³„: VectorDB 2ê°œ ì»¬ë ‰ì…˜ ë¡œë“œ/êµ¬ì¶• ì™„ë£Œ ---\")\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (í•¨ìˆ˜ 4/9) ---\n",
    "def call_gpt4o(chat_messages, current_profile):\n",
    "  \"\"\"(ë©”ì¸) gpt-4.1-mini APIë¥¼ í˜¸ì¶œí•˜ê³  JSON ì‘ë‹µì„ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "  \n",
    "  system_message_with_profile = f\"\"\"\n",
    "  {SYSTEM_PROMPT}\n",
    "  [í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ í”„ë¡œí•„]\n",
    "  {json.dumps(current_profile, indent=2, ensure_ascii=False)}\n",
    "  [ëŒ€í™” ê¸°ë¡]\n",
    "  (ëŒ€í™” ê¸°ë¡ì€ ì•„ë˜ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤)\n",
    "  \"\"\"\n",
    "  \n",
    "  messages_for_api = [\n",
    "    {\"role\": \"system\", \"content\": system_message_with_profile}\n",
    "  ]\n",
    "  messages_for_api.extend(chat_messages)\n",
    "\n",
    "  try:\n",
    "    response = client.chat.completions.create(\n",
    "      model=GPT_API_NAME,\n",
    "      messages=messages_for_api,\n",
    "      response_format={\"type\": \"json_object\"}, \n",
    "      temperature=0.7\n",
    "    )\n",
    "    \n",
    "    response_content = response.choices[0].message.content\n",
    "    response_data = json.loads(response_content)\n",
    "    \n",
    "    bot_message = response_data.get(\"bot_response\", \"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\")\n",
    "    updated_profile = response_data.get(\"updated_profile\", current_profile)\n",
    "    \n",
    "    return bot_message, updated_profile\n",
    "    \n",
    "  except Exception as e:\n",
    "    print(f\"API í˜¸ì¶œ ë˜ëŠ” JSON íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "    error_message = f\"ì£„ì†¡í•©ë‹ˆë‹¤. ì±—ë´‡ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\"\n",
    "    return error_message, current_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (í•¨ìˆ˜ 5/9) ---\n",
    "def start_chat():\n",
    "  \"\"\"\n",
    "  ì±„íŒ…ë°©ì´ ì²˜ìŒ ë¡œë“œë  ë•Œ ì‹¤í–‰.\n",
    "  gpt-4.1-minië¥¼ í˜¸ì¶œí•˜ì—¬ ì²« ë²ˆì§¸ ì§ˆë¬¸ì„ ë°›ëŠ”ë‹¤.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    initial_profile = PROFILE_TEMPLATE.copy()\n",
    "    \n",
    "    bot_message, updated_profile = call_gpt4o(\n",
    "      chat_messages=[], \n",
    "      current_profile=initial_profile\n",
    "    )\n",
    "    \n",
    "    # (messages í˜•ì‹ ë°˜í™˜)\n",
    "    gradio_history = [{\"role\": \"assistant\", \"content\": bot_message}]\n",
    "    llm_history = [{\"role\": \"assistant\", \"content\": bot_message}]\n",
    "    \n",
    "    return gradio_history, llm_history, updated_profile, False\n",
    "\n",
    "  except Exception as e:\n",
    "    print(f\"start_chatì—ì„œ API í˜¸ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "    error_msg = f\"ì±—ë´‡ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (API í‚¤ ì˜¤ë¥˜ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤): {e}\"\n",
    "    return [{\"role\": \"assistant\", \"content\": error_msg}], [], PROFILE_TEMPLATE.copy(), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (í•¨ìˆ˜ 6/9) ---\n",
    "def generate_profile_summary(profile_data):\n",
    "  \"\"\"\n",
    "  ì™„ì„±ëœ í”„ë¡œí•„(JSON)ì„ ë°›ì•„, gpt-4.1-minië¥¼ í˜¸ì¶œí•˜ì—¬\n",
    "  (1) Gradio ì±„íŒ…ìš© ë©”ì‹œì§€, (2) CSV ì €ì¥ìš© ì›ë³¸ ìš”ì•½ë¬¸ í…ìŠ¤íŠ¸ \n",
    "  2ê°€ì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "  \"\"\"\n",
    "  profile_str = json.dumps(profile_data, indent=2, ensure_ascii=False)\n",
    "  \n",
    "  summary_system_prompt = \"\"\"\n",
    "  ë‹¹ì‹ ì€ JSON í”„ë¡œí•„ ë°ì´í„°ë¥¼ ë°›ì•„ì„œ, ê·¸ ì‚¬ëŒì˜ ì…ì¥ì—ì„œ ìì‹ ì„ ì†Œê°œí•˜ëŠ” 'êµ¬ì–´ì²´' í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ê¸€ì“°ê¸° ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "  [ê·œì¹™]\n",
    "  1. (í•„ìˆ˜) JSONì˜ 'name' í•„ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ \"ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” [name]ì…ë‹ˆë‹¤.\"ë¡œ ë¬¸ì¥ì„ ì‹œì‘í•˜ì„¸ìš”.\n",
    "  2. ë”±ë”±í•œ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ, í•˜ë‚˜ì˜ ì—°ê²°ëœ ë¬¸ë‹¨ìœ¼ë¡œ ë§Œë“œì„¸ìš”.\n",
    "  3. ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•˜ë˜, ìì—°ìŠ¤ëŸ½ê²Œ ë¬¸ì¥ì— ë…¹ì—¬ë‚´ì„¸ìš”.\n",
    "  4. 'party_size'ì™€ 'travel_type'ì„ ë¬¶ì–´ì„œ í‘œí˜„í•˜ì„¸ìš”.\n",
    "  5. 'budget'ì€ \"ê°€ì„±ë¹„ ìˆëŠ”(ì €ë ´í•œ)\", \"ì ë‹¹í•œ\", \"ê³ ê¸‰ìŠ¤ëŸ¬ìš´\" ë“±ìœ¼ë¡œ í‘œí˜„í•˜ì„¸ìš”.\n",
    "  \"\"\"\n",
    "  \n",
    "  user_prompt = f\"\"\"\n",
    "  [ì‚¬ìš©ì í”„ë¡œí•„ JSON]\n",
    "  {profile_str}\n",
    "  ìœ„ í”„ë¡œí•„ì„ ë°”íƒ•ìœ¼ë¡œ ê·œì¹™ì— ë§ê²Œ ìê¸°ì†Œê°œ ê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\n",
    "  \"\"\"\n",
    "  \n",
    "  try:\n",
    "    response = client.chat.completions.create(\n",
    "      model=GPT_API_NAME,\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": summary_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "      ],\n",
    "      temperature=0.7\n",
    "    )\n",
    "    \n",
    "    raw_summary_text = response.choices[0].message.content\n",
    "    name = profile_data.get('name', 'ì‚¬ìš©ì')\n",
    "    chat_message_html = f\"\\n\\n---\\n\\n### ğŸ¤– AIê°€ íŒŒì•…í•œ {name}ë‹˜ì˜ í”„ë¡œí•„\\n\\n{raw_summary_text}\"\n",
    "    \n",
    "    return chat_message_html, raw_summary_text\n",
    "  \n",
    "  except Exception as e:\n",
    "    print(f\"ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}\")\n",
    "    error_html = \"\\n\\n(í”„ë¡œí•„ ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.)\"\n",
    "    error_text = \"(í”„ë¡œí•„ ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.)\"\n",
    "    return error_html, error_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (í•¨ìˆ˜ 7/9) ---\n",
    "def create_filter_metadata(profile_data):\n",
    "  \"\"\"\n",
    "  13ê°œ í•­ëª©ì˜ ì „ì²´ í”„ë¡œí•„ì„ ë°›ì•„,\n",
    "  í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì— í•„ìš”í•œ 6ê°œ í•­ëª©ì˜ í•„í„° ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "  \"\"\"\n",
    "  filter_dict = {\n",
    "    \"budget_range\": profile_data.get('budget', 'N/A'),\n",
    "    \"spicy_available\": profile_data.get('spicy_ok', 'N/A'),\n",
    "    \"vegetarian_options\": profile_data.get('is_vegetarian', 'N/A'),\n",
    "    \"main_ingredients_list\": profile_data.get('like_ingredients', 'N/A'),\n",
    "    \"suitable_for\": profile_data.get('travel_type', 'N/A'),\n",
    "    \"food_category\": profile_data.get('food_category', 'N/A')\n",
    "  }\n",
    "  return filter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (í•¨ìˆ˜ 8/9): í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë¡œì§ (ì´ 3ê°œ í•¨ìˆ˜) ---\n",
    "def generate_rag_query(user_profile_summary):\n",
    "  \"\"\"\n",
    "  LLMì„ í˜¸ì¶œí•˜ì—¬ ê¸´ ìê¸°ì†Œê°œ(ìš”ì•½ë¬¸)ë¥¼\n",
    "  ê°€ê²Œ RAG í…ìŠ¤íŠ¸ì™€ ë§¤ì¹­í•˜ê¸° ì¢‹ì€ 'ì§§ì€ í•µì‹¬ ì¿¼ë¦¬'ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "  \"\"\"\n",
    "  print(\"  > [RAG] LLMì„ í˜¸ì¶œí•˜ì—¬ 'ë¶„ìœ„ê¸°/ì„±í–¥' ì¿¼ë¦¬ë¥¼ ì¬ì‘ì„±í•©ë‹ˆë‹¤...\")\n",
    "  \n",
    "  system_prompt = \"\"\"\n",
    "  ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê¸´ ìê¸°ì†Œê°œ í…ìŠ¤íŠ¸ë¥¼, ë ˆìŠ¤í† ë‘ ë²¡í„° DBì—ì„œ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ\n",
    "  'ì§§ê³  í•µì‹¬ì ì¸ ì¿¼ë¦¬ ë¬¸ì¥'ìœ¼ë¡œ ì¬ì‘ì„±(Re-writing)í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "  \n",
    "  [ê·œì¹™]\n",
    "  1.  'ì•ˆë…•í•˜ì„¸ìš”', 'ì €ëŠ” OOOì…ë‹ˆë‹¤', '30ëŒ€', 'ìºë‚˜ë‹¤' ë“± ê°œì¸ ì‹ ìƒ ì •ë³´ëŠ” *ëª¨ë‘ ì œê±°*í•©ë‹ˆë‹¤.\n",
    "  2.  'ì˜ˆì‚°(ì €/ì¤‘/ê³ )', 'ë§µê¸°(O/X)', 'ì„ í˜¸ ì¬ë£Œ(ì†Œê³ ê¸°)' ë“± 'ì‚¬ì‹¤(Fact)' ì •ë³´ëŠ” *ëª¨ë‘ ì œê±°*í•©ë‹ˆë‹¤.\n",
    "  3.  ì˜¤ì§ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” *ë¶„ìœ„ê¸°*, *ìƒí™©*, *ê²½í—˜*, *ì„±í–¥* (ì˜ˆ: 'ì¡°ìš©í•œ', 'í˜¼ì', 'ì—°ì¸ê³¼ í•¨ê»˜', 'ìƒˆë¡œìš´ ë„ì „', 'ì¸ê¸° ë§›ì§‘', 'ê°€ì¡±ì ì¸')ë§Œ ì¶”ì¶œí•˜ì—¬ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.\n",
    "  4.  ê²°ê³¼ëŠ” ì˜¤ì§ 'ì¬ì‘ì„±ëœ ì¿¼ë¦¬ ë¬¸ì¥' í•˜ë‚˜ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "  \"\"\"\n",
    "  \n",
    "  user_prompt = f\"\"\"\n",
    "  [ì‚¬ìš©ì ìê¸°ì†Œê°œ]\n",
    "  {user_profile_summary}\n",
    "  \n",
    "  [ì¬ì‘ì„±ëœ ì¿¼ë¦¬]\n",
    "  \"\"\"\n",
    "\n",
    "  try:\n",
    "    response = client.chat.completions.create(\n",
    "      model=GPT_API_NAME,\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "      ],\n",
    "      temperature=0.2\n",
    "    )\n",
    "    rewritten_query = response.choices[0].message.content.strip().replace('\"', '')\n",
    "    return rewritten_query\n",
    "  except Exception as e:\n",
    "    print(f\"  > [ì˜¤ë¥˜] ì¿¼ë¦¬ ì¬ì‘ì„± ì‹¤íŒ¨: {e}\")\n",
    "    return user_profile_summary[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_filters_from_profile(user_filter_dict):\n",
    "  \"\"\"\n",
    "  ì‚¬ìš©ì í”„ë¡œí•„ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°›ì•„ ChromaDB 1ì°¨ í•„í„°(DB)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "  \"\"\"\n",
    "  db_pre_filter_list = [] \n",
    "  \n",
    "  DB_FILTER_KEYS = ['budget_range', 'spicy_available', 'vegetarian_options']\n",
    "\n",
    "  for key, value in user_filter_dict.items():\n",
    "    if value == 'N/A' or not value: \n",
    "      continue\n",
    "      \n",
    "    if key == 'food_category':\n",
    "      # ì‚¬ìš©ìì˜ 'food_category'ëŠ” ê°€ê²Œ DBì˜ 'high_level_category'ì™€ ë§¤ì¹­\n",
    "      db_pre_filter_list.append({\"high_level_category\": value})\n",
    "      \n",
    "    elif key in DB_FILTER_KEYS:\n",
    "      db_pre_filter_list.append({key: value})\n",
    "      \n",
    "  db_pre_filter = {\"$and\": db_pre_filter_list} if db_pre_filter_list else {}\n",
    "  \n",
    "  return db_pre_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (14ë²ˆ ì…€ - format_restaurant_markdown í•¨ìˆ˜)\n",
    "\n",
    "# --- (í•¨ìˆ˜ 8/9): Markdown ì„œì‹ í—¬í¼ í•¨ìˆ˜ ---\n",
    "\n",
    "def format_restaurant_markdown(store_id_str, rank_prefix=\"ì¶”ì²œ\", rank_index=1):\n",
    "  \"\"\"\n",
    "  (ì‹ ê·œ í—¬í¼ í•¨ìˆ˜)\n",
    "  store_id_str(ê°€ê²ŒID)ì„(ë¥¼) ë°›ì•„, ì „ì—­ ë³€ìˆ˜(df_restaurants ë“±)ë¥¼ ì°¸ì¡°í•˜ì—¬\n",
    "  Gradioì— í‘œì‹œí•  ë‹¨ì¼ ì‹ë‹¹ì˜ Markdown ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "  (ì½”ë“œ ì¤‘ë³µ ì œê±°ìš©)\n",
    "  \"\"\"\n",
    "  \n",
    "  # (ì „ì—­ ë³€ìˆ˜ ì°¸ì¡°)\n",
    "  global df_restaurants, menu_groups, df_restaurant_ratings_summary\n",
    "  \n",
    "  try:\n",
    "    # 1. (ê°€ê²Œ ì •ë³´ ì¡°íšŒ)\n",
    "    store_info = df_restaurants.loc[store_id_str]\n",
    "    store_name = store_info['ê°€ê²Œ']\n",
    "    store_address = store_info['ì£¼ì†Œ']\n",
    "    store_intro = store_info['ì†Œê°œ']\n",
    "    store_image_url = store_info.get('ì´ë¯¸ì§€URL', '') \n",
    "    \n",
    "    # (â˜…â˜…â˜… 1-1. ì‹ ê·œ ë§í¬ìš© ë°ì´í„° ì¶”ì¶œ â˜…â˜…â˜…)\n",
    "    detail_url = store_info.get('ìƒì„¸URL', '')\n",
    "    store_y = store_info.get('Yì¢Œí‘œ', '')\n",
    "    store_x = store_info.get('Xì¢Œí‘œ', '')\n",
    "    \n",
    "    try:\n",
    "      store_category = store_info.get('high_level_category', 'N/A')\n",
    "    except KeyError:\n",
    "      store_category = 'N/A' \n",
    "\n",
    "    # 2. (ë‹¤ë¥¸ ì‚¬ìš©ì í‰ê°€ ì¹´ìš´íŠ¸ ì¡°íšŒ)\n",
    "    social_proof_string = \"\" \n",
    "    if df_restaurant_ratings_summary is not None and not df_restaurant_ratings_summary.empty:\n",
    "      try:\n",
    "        rating_info = df_restaurant_ratings_summary[\n",
    "          df_restaurant_ratings_summary['restaurant_id'] == store_id_str\n",
    "        ]\n",
    "        if not rating_info.empty:\n",
    "          recommend_count = rating_info['ì¶”ì²œ'].iloc[0]\n",
    "          non_recommend_count = rating_info['ë¯¸ì¶”ì²œ'].iloc[0]\n",
    "          social_proof_string = (\n",
    "            f\"**ë‹¤ë¥¸ ì‚¬ìš©ì í‰ê°€:** ğŸ‘ {recommend_count}ëª… / ğŸ‘ {non_recommend_count}ëª…\\n\\n\"\n",
    "          )\n",
    "      except Exception as e:\n",
    "        print(f\"[ì„œì‹ ì˜¤ë¥˜] ID {store_id_str} í‰ê°€ ì¹´ìš´íŠ¸ ì¡°íšŒ: {e}\")\n",
    "\n",
    "    # 3. (ì´ë¯¸ì§€ ë§ˆí¬ë‹¤ìš´ ìƒì„±)\n",
    "    image_md_string = \"\"\n",
    "    no_image_filename = \"img_restaruant_no_image.png\"\n",
    "    if pd.notna(store_image_url) and store_image_url:\n",
    "      path = urlparse(store_image_url).path\n",
    "      filename = os.path.basename(path)\n",
    "      if filename != no_image_filename:\n",
    "        image_md_string = f\"![{store_name} ì´ë¯¸ì§€]({store_image_url})\\n\\n\"\n",
    "        \n",
    "    # (â˜…â˜…â˜… 4. ì‹ ê·œ ë§í¬ 2ì¢… ìƒì„± (HTML ì‚¬ìš©) â˜…â˜…â˜…)\n",
    "    detail_link_md = \"\"\n",
    "    if pd.notna(detail_url) and detail_url:\n",
    "      # (Gradioì—ì„œ ìƒˆ íƒ­ìœ¼ë¡œ ì—´ê¸° ìœ„í•´ HTML <a> íƒœê·¸ ì‚¬ìš©)\n",
    "      detail_link_md = f'<a href=\"{detail_url}\" target=\"_blank\">ê°€ê²Œ ìƒì„¸ì •ë³´</a>'\n",
    "\n",
    "    map_link_md = \"\"\n",
    "    if pd.notna(store_y) and pd.notna(store_x) and store_y and store_x:\n",
    "      # (ê°€ê²Œ ì´ë¦„ì— íŠ¹ìˆ˜ë¬¸ì/ê³µë°±ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ URL ì¸ì½”ë”©)\n",
    "      store_name_encoded = quote(store_name)\n",
    "      kakao_map_url = f\"https://map.kakao.com/?q={store_name_encoded}&map_type=TYPE_MAP&rq={store_y},{store_x}\"\n",
    "      map_link_md = f'<a href=\"{kakao_map_url}\" target=\"_blank\">ì¹´ì¹´ì˜¤ë§µ ê¸¸ì°¾ê¸°</a>'\n",
    "\n",
    "    # (ë‘ ë§í¬ë¥¼ ì¡°í•©)\n",
    "    links_md = \"\"\n",
    "    if detail_link_md and map_link_md:\n",
    "      links_md = f\"{detail_link_md} | {map_link_md}\\n\\n\"\n",
    "    elif detail_link_md:\n",
    "      links_md = f\"{detail_link_md}\\n\\n\"\n",
    "    elif map_link_md:\n",
    "      links_md = f\"{map_link_md}\\n\\n\"\n",
    "    # (â˜…â˜…â˜… ë§í¬ ìƒì„± ë â˜…â˜…â˜…)\n",
    "\n",
    "    # 5. (ë©”ë‰´ ì •ë³´ ì¡°íšŒ)\n",
    "    menu_str = \"\"\n",
    "    try:\n",
    "      menus_df = menu_groups.get_group(store_id_str)\n",
    "      rep_menus = menus_df[menus_df['ëŒ€í‘œì—¬ë¶€'] == 'Y'].head(3)\n",
    "      if rep_menus.empty:\n",
    "        rep_menus = menus_df.head(3)\n",
    "      for _, menu_row in rep_menus.iterrows():\n",
    "        menu_str += f\"* {menu_row['ë©”ë‰´']} ({menu_row['ê°€ê²©ì›ë¬¸']})\\n\"\n",
    "      if not menu_str:\n",
    "        menu_str = \"* (ë©”ë‰´ ì •ë³´ ì—†ìŒ)\\n\"\n",
    "    except KeyError:\n",
    "      menu_str = \"* (ë©”ë‰´ ì •ë³´ ì—†ìŒ)\\n\"\n",
    "\n",
    "    # 6. (â˜…â˜…â˜… ìµœì¢… Markdown ì¡°í•© - links_md ì¶”ê°€ â˜…â˜…â˜…)\n",
    "    output_md = (\n",
    "      f\"**[{rank_prefix} {rank_index}] {store_name}**\\n\\n\"\n",
    "      f\"{image_md_string}\"\n",
    "      f\"{social_proof_string}\"\n",
    "      f\"{links_md}\" # (â˜…â˜…â˜… ë§í¬ ë¬¸ìì—´ ì‚½ì… â˜…â˜…â˜…)\n",
    "      f\"**ìœ„ì¹˜:** {store_address}\\n\\n\"\n",
    "      f\"**ì†Œê°œ:** {store_intro}\\n\\n\"\n",
    "      f\"**ìŒì‹ì¢…ë¥˜:** {store_category}\\n\\n\"\n",
    "      f\"**ì£¼ìš”ë©”ë‰´:**\\n{menu_str}\\n\"\n",
    "      f\"\\n---\\n\\n\"\n",
    "    )\n",
    "    return output_md\n",
    "    \n",
    "  except KeyError as ke:\n",
    "     print(f\"[ì„œì‹ ì˜¤ë¥˜] ID {store_id_str} (KeyError): {ke}\")\n",
    "     return f\"**[{rank_prefix} {rank_index}] ID: {store_id_str}** (ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨)\\n\\n---\\n\\n\"\n",
    "  except Exception as inner_e:\n",
    "     print(f\"[ì„œì‹ ì˜¤ë¥˜] ID {store_id_str} (Exception): {inner_e}\")\n",
    "     return f\"**[{rank_prefix} {rank_index}] ID: {store_id_str}** (ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨)\\n\\n---\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (â˜…â˜…â˜… ì‹ ê·œ ì…€ â˜…â˜…â˜…)\n",
    "# --- (í•¨ìˆ˜ 8/9): ìœ ì‚¬ ì‚¬ìš©ì ì¶”ì²œ í—¬í¼ í•¨ìˆ˜ ---\n",
    "\n",
    "def get_similar_user_recommendations(\n",
    "    live_rag_query_text, \n",
    "    primary_reco_ids, \n",
    "    max_similar_users=1, \n",
    "    max_new_recos=2\n",
    "  ):\n",
    "  \"\"\"\n",
    "  (ì‹ ê·œ í—¬í¼ í•¨ìˆ˜)\n",
    "  í˜„ì¬ ì‚¬ìš©ìì˜ RAG ì¿¼ë¦¬ì™€ ê¸°ë³¸ ì¶”ì²œ ID ëª©ë¡ì„ ë°›ì•„,\n",
    "  ìœ ì‚¬ ì‚¬ìš©ìê°€ 'ì¶”ì²œ'í•œ ì‹ë‹¹ ì¤‘ ê²¹ì¹˜ì§€ ì•ŠëŠ” ì‹ë‹¹ì˜\n",
    "  Markdown ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "  \"\"\"\n",
    "  \n",
    "  # (ì „ì—­ ë³€ìˆ˜ ì°¸ì¡°)\n",
    "  global profile_collection, df_all_user_ratings\n",
    "  \n",
    "  if profile_collection is None:\n",
    "    print(\"[ìœ ì‚¬ ì¶”ì²œ] 'profile_collection'ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    return \"\"\n",
    "    \n",
    "  if df_all_user_ratings is None:\n",
    "    print(\"[ìœ ì‚¬ ì¶”ì²œ] 'df_all_user_ratings'ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    return \"\"\n",
    "\n",
    "  try:\n",
    "    # 1. 'mock_profiles' DBì—ì„œ ìœ ì‚¬ ì‚¬ìš©ì ì¿¼ë¦¬\n",
    "    results = profile_collection.query(\n",
    "      query_texts=[live_rag_query_text],\n",
    "      n_results=max_similar_users\n",
    "    )\n",
    "    \n",
    "    if not results.get('ids', [[]])[0]:\n",
    "      print(\"[ìœ ì‚¬ ì¶”ì²œ] ìœ ì‚¬í•œ ì‚¬ìš©ìë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "      return \"\"\n",
    "      \n",
    "    # 2. ìœ ì‚¬ ì‚¬ìš©ìì˜ user_id ì¶”ì¶œ\n",
    "    similar_user_ids = [meta['user_id'] for meta in results['metadatas'][0]]\n",
    "    print(f\"[ìœ ì‚¬ ì¶”ì²œ] ì°¾ì€ ìœ ì‚¬ ì‚¬ìš©ì: {similar_user_ids}\")\n",
    "\n",
    "    # 3. ìœ ì‚¬ ì‚¬ìš©ìê°€ 'ì¶”ì²œ'í•œ ì‹ë‹¹ ID ëª©ë¡ ì¡°íšŒ\n",
    "    similar_user_likes = df_all_user_ratings[\n",
    "      (df_all_user_ratings['user_id'].isin(similar_user_ids)) &\n",
    "      (df_all_user_ratings['ì‚¬ìš©ìí‰ê°€'] == 'ì¶”ì²œ')\n",
    "    ]\n",
    "    \n",
    "    if similar_user_likes.empty:\n",
    "      print(\"[ìœ ì‚¬ ì¶”ì²œ] ìœ ì‚¬ ì‚¬ìš©ìê°€ 'ì¶”ì²œ'í•œ ì‹ë‹¹ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "      return \"\"\n",
    "\n",
    "    # 4. ê¸°ë³¸ ì¶”ì²œê³¼ ê²¹ì¹˜ì§€ ì•ŠëŠ” ì‹ë‹¹ ID í•„í„°ë§\n",
    "    new_recommendations = []\n",
    "    for store_id in similar_user_likes['restaurant_id'].astype(str):\n",
    "      if store_id not in primary_reco_ids and store_id not in new_recommendations:\n",
    "        new_recommendations.append(store_id)\n",
    "        \n",
    "    if not new_recommendations:\n",
    "      print(\"[ìœ ì‚¬ ì¶”ì²œ] ê²¹ì¹˜ì§€ ì•ŠëŠ” ì¶”ê°€ ì¶”ì²œ ì‹ë‹¹ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "      return \"\"\n",
    "      \n",
    "    # 5. ìµœì¢… Markdown ë¬¸ìì—´ ìƒì„± (êµ¬ë¶„ì í¬í•¨)\n",
    "    output_secondary_string = (\n",
    "      f\"\\n\\n---\\n\\n\"\n",
    "      f\"### ğŸ¤– Charlieë‹˜ê³¼ ë¹„ìŠ·í•œ ì‚¬ìš©ìê°€ ì¶”ì²œí•œ ì‹ë‹¹\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    # (ìƒˆë¡œ ì°¾ì€ ì¶”ì²œ ì‹ë‹¹ ì¤‘ ìƒìœ„ Nê°œë§Œ)\n",
    "    recos_to_show = new_recommendations[:max_new_recos]\n",
    "    print(f\"[ìœ ì‚¬ ì¶”ì²œ] ì¶”ê°€í•  ì‹ë‹¹: {recos_to_show}\")\n",
    "    \n",
    "    for i, store_id in enumerate(recos_to_show):\n",
    "      # (ìœ„ì—ì„œ ë§Œë“  14.5ë²ˆ ì…€ì˜ í—¬í¼ í•¨ìˆ˜ ì‚¬ìš©)\n",
    "      output_secondary_string += format_restaurant_markdown(\n",
    "        store_id, \n",
    "        rank_prefix=\"ìœ ì‚¬ ì¶”ì²œ\", \n",
    "        rank_index=i+1\n",
    "      )\n",
    "      \n",
    "    return output_secondary_string\n",
    "    \n",
    "  except Exception as e:\n",
    "    print(f\"[ì˜¤ë¥˜] ìœ ì‚¬ ì‚¬ìš©ì ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    return \"\" # (ì˜¤ë¥˜ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (â˜…â˜…â˜… ìˆ˜ì •ë¨ â˜…â˜…â˜…)\n",
    "# --- 6. í—¬í¼ í•¨ìˆ˜ (5/7): í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰ (ì ìˆ˜ì œ) ---\n",
    "def run_hybrid_search_with_scoring(user_profile_row, n_results=5):\n",
    "  \"\"\"\n",
    "  (â˜…â˜…â˜… ìˆ˜ì •ë¨ â˜…â˜…â˜…)\n",
    "  RAG-First ê²€ìƒ‰ í›„, 'format_restaurant_markdown' í—¬í¼ë¥¼ ì‚¬ìš©í•˜ì—¬\n",
    "  ê¸°ë³¸ ì¶”ì²œ 5ê°œë¥¼ ìƒì„±í•˜ê³ , 'get_similar_user_recommendations' í—¬í¼ë¥¼\n",
    "  í˜¸ì¶œí•˜ì—¬ ìœ ì‚¬ ì‚¬ìš©ì ì¶”ì²œì„ ë§ë¶™ì…ë‹ˆë‹¤.\n",
    "  \"\"\"\n",
    "  print(\"\\n--- 3ë‹¨ê³„: RAG + ì ìˆ˜ì œ(Scoring) ê²€ìƒ‰ ì‹œì‘ ---\")\n",
    "  \n",
    "  # 1. ì‚¬ìš©ì í”„ë¡œí•„(Pandas Row)ì—ì„œ ë°ì´í„° ì¶”ì¶œ\n",
    "  try:\n",
    "    user_original_summary = user_profile_row['rag_query_text']\n",
    "    user_filter_dict = ast.literal_eval(user_profile_row['filter_metadata_json'])\n",
    "  except Exception as e:\n",
    "    print(f\"[ì˜¤ë¥˜] ì‚¬ìš©ì í”„ë¡œí•„ íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "    return \"(ì˜¤ë¥˜: ì‚¬ìš©ì í”„ë¡œí•„ íŒŒì‹± ì‹¤íŒ¨)\"\n",
    "\n",
    "  # 2. (í•µì‹¬) ì¿¼ë¦¬ ë° í•„í„° ìƒì„±\n",
    "  user_rag_query = generate_rag_query(user_original_summary)\n",
    "  db_pre_filter = build_filters_from_profile(user_filter_dict)\n",
    "  python_post_filter = {key: val.split(',') for key, val in user_filter_dict.items() \n",
    "                        if key in ['main_ingredients_list', 'suitable_for'] and val != 'N/A' and val}\n",
    "  \n",
    "  REQUEST_N_RESULTS = 50 # (RAG ê²€ìƒ‰ìœ¼ë¡œ 50ê°œë¥¼ ë¨¼ì € í™•ë³´)\n",
    "\n",
    "  print(f\"í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì: {user_profile_row['name']} ({user_profile_row['user_id']})\")\n",
    "  print(f\"RAG ì¿¼ë¦¬ (ì¬ì‘ì„±ë¨): '{user_rag_query}'\")\n",
    "  print(f\"DB 1ì°¨ í•„í„° (ChromaDB): {db_pre_filter}\")\n",
    "  print(f\"Python 2ì°¨ í•„í„° (ì ìˆ˜ ê³„ì‚°ìš©): {python_post_filter}\")\n",
    "\n",
    "  # 3. (í•µì‹¬) ChromaDBì— RAG ê²€ìƒ‰ ì‹¤í–‰\n",
    "  try:\n",
    "    print(f\"\\n--- 1ë‹¨ê³„: RAG + 1ì°¨ í•„í„° ê²€ìƒ‰ | Top {REQUEST_N_RESULTS}ê°œ ì°¾ê¸° ---\")\n",
    "    \n",
    "    if db_pre_filter: \n",
    "      results = collection.query(\n",
    "        query_texts=[user_rag_query],\n",
    "        n_results=REQUEST_N_RESULTS,\n",
    "        where=db_pre_filter\n",
    "      )\n",
    "    else: \n",
    "      results = collection.query(\n",
    "        query_texts=[user_rag_query],\n",
    "        n_results=REQUEST_N_RESULTS\n",
    "      )\n",
    "    \n",
    "    print(f\"--- 1ì°¨ ê²€ìƒ‰ ì™„ë£Œ: {len(results['ids'][0])}ê°œ í›„ë³´ ë°˜í™˜ ---\")\n",
    "    \n",
    "    # (í•„í„° ì™„í™” ë¡œì§)\n",
    "    if not results.get('ids', [[]])[0]:\n",
    "      print(\"  > [í•„í„° ì™„í™”] 1ì°¨ í•„í„° ê²°ê³¼ 0ê±´. RAG-Only(í•„í„° ì—†ìŒ)ë¡œ ì¬ì‹œë„...\")\n",
    "      results = collection.query(\n",
    "        query_texts=[user_rag_query],\n",
    "        n_results=REQUEST_N_RESULTS\n",
    "      )\n",
    "      print(f\"  > RAG-Only ê²€ìƒ‰ ì™„ë£Œ: {len(results['ids'][0])}ê°œ í›„ë³´ ë°˜í™˜\")\n",
    "      if not results.get('ids', [[]])[0]:\n",
    "        print(\"RAG-Only ê²€ìƒ‰ ê²°ê³¼ë„ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return \"\\n\\n(ê²€ìƒ‰ ê²°ê³¼: 1ì°¨ í•„í„° ë° RAG-Only ê²€ìƒ‰ ëª¨ë‘ 0ê±´)\"\n",
    "        \n",
    "    # 4. Pythonìœ¼ë¡œ *ì ìˆ˜(Scoring)* ê³„ì‚°\n",
    "    final_results_with_score = []\n",
    "    print(\"\\n--- 2ë‹¨ê³„: ì ìˆ˜(Scoring) ê³„ì‚° ì‹œì‘ (1ì°¨ í›„ë³´ ëŒ€ìƒ) ---\")\n",
    "    \n",
    "    for i in range(len(results['ids'][0])):\n",
    "      store_id = results['ids'][0][i]\n",
    "      rag_distance = results['distances'][0][i] \n",
    "      metadata = results['metadatas'][0][i]\n",
    "      \n",
    "      filter_score = 0\n",
    "      \n",
    "      if user_filter_dict.get('food_category') == metadata.get('high_level_category'):\n",
    "        filter_score += 3\n",
    "      if user_filter_dict.get('budget_range') == metadata.get('budget_range'):\n",
    "        filter_score += 2\n",
    "      if user_filter_dict.get('spicy_available') == metadata.get('spicy_available'):\n",
    "        filter_score += 2\n",
    "      if user_filter_dict.get('vegetarian_options') == metadata.get('vegetarian_options'):\n",
    "        filter_score += 2\n",
    "\n",
    "      if 'suitable_for' in python_post_filter:\n",
    "        if all(req in metadata.get('suitable_for', '') for req in python_post_filter['suitable_for']): \n",
    "          filter_score += 1\n",
    "      if 'main_ingredients_list' in python_post_filter:\n",
    "        if any(req in metadata.get('main_ingredients_list', '') for req in python_post_filter['main_ingredients_list']): \n",
    "          filter_score += 1\n",
    "\n",
    "      final_results_with_score.append({\n",
    "        \"id\": store_id,\n",
    "        \"rag_distance\": rag_distance, \n",
    "        \"filter_score\": filter_score, \n",
    "        \"metadata\": metadata\n",
    "      })\n",
    "    \n",
    "    # 5. ìµœì¢… ë­í‚¹\n",
    "    final_results = sorted(\n",
    "      final_results_with_score, \n",
    "      key=lambda x: (-x['filter_score'], x['rag_distance']), \n",
    "    )[:n_results]\n",
    "    \n",
    "    # 6. (â˜…â˜…â˜… UI í¬ë§· ìˆ˜ì • â˜…â˜…â˜…)\n",
    "    print(\"\\n--- 3ë‹¨ê³„: ìµœì¢… RAG + ì ìˆ˜ì œ ë­í‚¹ ê²°ê³¼ (ê¸°ë³¸ ì¶”ì²œ) ---\")\n",
    "    if not final_results:\n",
    "      return \"\\n\\n(ê²€ìƒ‰ ê²°ê³¼: 1/2ì°¨ í•„í„°ë¥¼ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” ì‹ë‹¹ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.)\"\n",
    "\n",
    "    output_string = \"\\n\\n---\\n\\n### ğŸ¤– Charlieë‹˜ì„ ìœ„í•œ ë§ì¶¤ ì‹ë‹¹ ì¶”ì²œ!\\n\\n\"\n",
    "    primary_reco_ids = set() # (ìœ ì‚¬ ì¶”ì²œê³¼ ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ID ì„¸íŠ¸)\n",
    "\n",
    "    for i, item in enumerate(final_results):\n",
    "      store_id_str = item['id']\n",
    "      primary_reco_ids.add(store_id_str)\n",
    "      \n",
    "      # (â˜…â˜…â˜… ì‹ ê·œ í—¬í¼ í•¨ìˆ˜(14.5ë²ˆ ì…€)ë¥¼ ì‚¬ìš©í•˜ì—¬ Markdown ìƒì„± â˜…â˜…â˜…)\n",
    "      output_string += format_restaurant_markdown(\n",
    "        store_id_str, \n",
    "        rank_prefix=\"ì¶”ì²œ\", \n",
    "        rank_index=i+1\n",
    "      )\n",
    "      \n",
    "    # 7. (â˜…â˜…â˜… 7. (ì‹ ê·œ) ìœ ì‚¬ ì‚¬ìš©ì ì¶”ì²œ ë¡œì§ ì¶”ê°€ â˜…â˜…â˜…)\n",
    "    try:\n",
    "      print(\"\\n--- 4ë‹¨ê³„: ìœ ì‚¬ ì‚¬ìš©ì ê¸°ë°˜ ì¶”ê°€ ì¶”ì²œ ê²€ìƒ‰ ---\")\n",
    "      # (ì‹ ê·œ í—¬í¼ í•¨ìˆ˜(14.6ë²ˆ ì…€) í˜¸ì¶œ)\n",
    "      secondary_reco_string = get_similar_user_recommendations(\n",
    "        user_original_summary, # (RAG í…ìŠ¤íŠ¸ ì›ë³¸ ì „ë‹¬)\n",
    "        primary_reco_ids       # (ì¤‘ë³µ ì œê±°ìš© ID ëª©ë¡ ì „ë‹¬)\n",
    "      )\n",
    "      if secondary_reco_string:\n",
    "        output_string += secondary_reco_string # (ê²°ê³¼ ë§ë¶™ì´ê¸°)\n",
    "    except Exception as e:\n",
    "      print(f\"[ê²½ê³ ] ìœ ì‚¬ ì‚¬ìš©ì ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    # (â˜…â˜…â˜… ë¡œì§ ë â˜…â˜…â˜…)\n",
    "\n",
    "  except Exception as e:\n",
    "    print(f\"\\n[ì˜¤ë¥˜] ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    return f\"\\n\\n(ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e})\"\n",
    "  \n",
    "  return output_string # (ìµœì¢… Markdown ë°˜í™˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. í—¬í¼ í•¨ìˆ˜ (Gradio ì½”ì–´) ---\n",
    "def chat_survey(message, gradio_history, llm_history, current_profile, is_completed):\n",
    "  \"\"\"\n",
    "  ì‚¬ìš©ìê°€ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•  ë•Œë§ˆë‹¤ ì‹¤í–‰ë˜ëŠ” ë©”ì¸ í•¨ìˆ˜\n",
    "  (ì™„ë£Œ ì‹œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰ ë° 2ê°œ ì˜ì—­ìœ¼ë¡œ ë¶„ë¦¬ ì¶œë ¥)\n",
    "  \"\"\"\n",
    "  \n",
    "  # 1. ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ (Gradio UIìš©)\n",
    "  gradio_history.append({\"role\": \"user\", \"content\": message})\n",
    "  \n",
    "  # 2. ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ (LLM APIìš©)\n",
    "  llm_history.append({\"role\": \"user\", \"content\": message})\n",
    "  \n",
    "  # 3. gpt-4.1-mini API í˜¸ì¶œ (ì •ë³´ ìˆ˜ì§‘)\n",
    "  try:\n",
    "    bot_message, updated_profile = call_gpt4o(llm_history, current_profile)\n",
    "  except Exception as e:\n",
    "    print(f\"chat_surveyì—ì„œ API í˜¸ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "    error_msg = f\"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\"\n",
    "    gradio_history.append({\"role\": \"assistant\", \"content\": error_msg})\n",
    "    return gradio_history, llm_history, current_profile, is_completed, gr.update()\n",
    "\n",
    "  # 4. ë´‡ ì‘ë‹µ ì¶”ê°€ (LLM APIìš©)\n",
    "  llm_history.append({\"role\": \"assistant\", \"content\": bot_message})\n",
    "\n",
    "  # --- 5. ì™„ë£Œ ì—¬ë¶€ í™•ì¸ ë° ìµœì¢… ë°ì´í„° ìƒì„± ---\n",
    "  final_bot_message = bot_message\n",
    "  recommendation_string = gr.update() \n",
    "  \n",
    "  # í”„ë¡œí•„ì˜ ëª¨ë“  ê°’ì´ Noneì´ ì•„ë‹Œì§€ í™•ì¸\n",
    "  profile_is_complete = all(v is not None for v in updated_profile.values())\n",
    "  \n",
    "  if profile_is_complete and not is_completed:\n",
    "    print(\"--- í”„ë¡œí•„ ì™„ì„±! ìš”ì•½ ë° í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤. ---\")\n",
    "    gr.Info(\"í”„ë¡œí•„ì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤! AIê°€ ìš”ì•½ ë° ì‹ë‹¹ ì¶”ì²œì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...\")\n",
    "    \n",
    "    # (â˜… 1. êµ¬ì–´ì²´ ìš”ì•½ (RAG í…ìŠ¤íŠ¸) ìƒì„± - LLM í˜¸ì¶œ)\n",
    "    chat_message_html, raw_summary_text = generate_profile_summary(updated_profile)\n",
    "    \n",
    "    # (â˜… 2. í•„í„° ë©”íƒ€ë°ì´í„° ìƒì„± - Python ë³€í™˜)\n",
    "    filter_dict = create_filter_metadata(updated_profile)\n",
    "    filter_metadata_json = json.dumps(filter_dict, ensure_ascii=False)\n",
    "    \n",
    "    # (â˜… 3. RAG ì¿¼ë¦¬ ì¬ì‘ì„±ì„ ìœ„í•œ 'user_profile_row' ìƒì„±)\n",
    "    user_profile_row = {\n",
    "      \"name\": updated_profile.get(\"name\", \"N/A\"),\n",
    "      \"user_id\": \"live_user\",\n",
    "      \"rag_query_text\": raw_summary_text,\n",
    "      \"filter_metadata_json\": filter_metadata_json\n",
    "    }\n",
    "\n",
    "    # (â˜… 4. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰ â˜…â˜…â˜…)\n",
    "    if 'collection' in globals() and 'df_restaurants' in globals():\n",
    "      reco_md = run_hybrid_search_with_scoring(\n",
    "        user_profile_row\n",
    "      )\n",
    "      recommendation_string = gr.update(value=reco_md, visible=True)\n",
    "    else:\n",
    "      recommendation_string = gr.update(\n",
    "        value=\"\\n\\n[ì˜¤ë¥˜] DBê°€ ë¡œë“œë˜ì§€ ì•Šì•„ ì¶”ì²œì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\",\n",
    "        visible=True\n",
    "      )\n",
    "    \n",
    "    # (â˜…â˜…â˜… í„°ë¯¸ë„ ë¡œê·¸ ì¶œë ¥ â˜…â˜…â˜…)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" [í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìš© ë°ì´í„° ìƒì„± ì™„ë£Œ] \")\n",
    "    print(f\"\\n[1] name:\\n{user_profile_row['name']}\")\n",
    "    print(f\"\\n[2] rag_query_text:\\n{raw_summary_text.replace('\\n', ' ')}\")\n",
    "    print(f\"\\n[3] filter_metadata_json:\\n{filter_metadata_json}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    final_bot_message = f\"{bot_message}\\n{chat_message_html}\\n\\nğŸ‘‡ ì•„ë˜ì—ì„œ ì¶”ì²œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”! ğŸ‘‡\"\n",
    "    is_completed = True \n",
    "    print(json.dumps(updated_profile, indent=2, ensure_ascii=False))\n",
    "\n",
    "  # 6. Gradio ì±—ë´‡ ê¸°ë¡ ì—…ë°ì´íŠ¸ (UIìš©)\n",
    "  gradio_history.append({\"role\": \"assistant\", \"content\": final_bot_message})\n",
    "  \n",
    "  # 7. (5ê°œ ìƒíƒœ ë°˜í™˜)\n",
    "  return gradio_history, llm_history, updated_profile, is_completed, recommendation_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'restaurant_summaries_output_ALL.csv'ì—ì„œ ê°€ê²Œ DB ë¡œë“œ ì¤‘...\n",
      "ê°€ê²Œ DB 7713ê°œ ë¡œë“œ ì™„ë£Œ.\n",
      "'20251017_TOTAL_MENU.csv'ì—ì„œ ë©”ë‰´ DB ë¡œë“œ ì¤‘...\n",
      "ë©”ë‰´ DB 54625ê°œ ë¡œë“œ ì™„ë£Œ (ê·¸ë£¹í™” ì™„ë£Œ).\n",
      "'recommendation_results_with_ratings.csv'ì—ì„œ 500ëª… í‰ê°€ ë°ì´í„° ë¡œë“œ ì¤‘...\n",
      "  > ì‹ë‹¹(restaurant_id)ë³„ 'ì¶”ì²œ', 'ë¯¸ì¶”ì²œ' ì¹´ìš´íŠ¸ ì§‘ê³„ ì¤‘...\n",
      "  > 500ëª… í‰ê°€ ë°ì´í„° ì§‘ê³„ ì™„ë£Œ: 428ê°œ ì‹ë‹¹\n",
      "\n",
      "--- 2ë‹¨ê³„: VectorDB êµ¬ì¶•/ë¡œë“œ ì‹œì‘ ---\n",
      "  > SentenceTransformer ëª¨ë¸ ('distiluse-base-multilingual-cased-v1')ì„ ì „ì—­ 'sentence_embedder'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n",
      "'./restaurant_db' ê²½ë¡œì—ì„œ Persistent DB í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...\n",
      "\n",
      "[1/2] ê¸°ì¡´ 'restaurants' ì»¬ë ‰ì…˜ ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤...\n",
      "  > 'restaurants' ë¡œë“œ ì™„ë£Œ: 7713ê°œ\n",
      "\n",
      "[2/2] ê¸°ì¡´ 'mock_profiles' ì»¬ë ‰ì…˜ ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤...\n",
      "  > 'mock_profiles' ë¡œë“œ ì™„ë£Œ: 500ê°œ\n",
      "--- 2ë‹¨ê³„: VectorDB 2ê°œ ì»¬ë ‰ì…˜ ë¡œë“œ/êµ¬ì¶• ì™„ë£Œ ---\n"
     ]
    }
   ],
   "source": [
    "# --- 8. (â˜…â˜…â˜… ì‹ ê·œ) DB ë° ë°ì´í„° ì „ì—­ ë¡œë“œ ---\n",
    "# (Gradio ì•±ì´ ì‹œì‘ë˜ê¸° ì „ì— DBì™€ ê°€ê²Œ ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë¡œë“œ)\n",
    "\n",
    "MOCK_USER_RATINGS_FILE = \"recommendation_results_with_ratings.csv\" # (â˜…â˜…â˜… íŒŒì¼ ì´ë¦„ ì •ì˜ â˜…â˜…â˜…)\n",
    "\n",
    "try:\n",
    "  # (1) ê°€ê²Œ/ë©”ë‰´ ë°ì´í„° ë¡œë“œ (df_restaurants, df_menus ì „ì—­ ë³€ìˆ˜ ìƒì„±)\n",
    "  data_loaded_ok = load_app_data(RESTAURANT_DB_FILE, MENU_DB_FILE)\n",
    "  \n",
    "  # (â˜…â˜…â˜… 2. 500ëª… í‰ê°€ ë°ì´í„° ë¡œë“œ ë° ì§‘ê³„ (ì‹ ê·œ ì¶”ê°€) â˜…â˜…â˜…)\n",
    "  ratings_loaded_ok = False\n",
    "  if data_loaded_ok: # (ê°€ê²Œ ë°ì´í„°ê°€ ë¡œë“œë˜ì–´ì•¼ ì‹¤í–‰)\n",
    "    try:\n",
    "      print(f\"'{MOCK_USER_RATINGS_FILE}'ì—ì„œ 500ëª… í‰ê°€ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "      # (ì „ì—­ ë³€ìˆ˜ì— í• ë‹¹)\n",
    "      df_all_user_ratings = pd.read_csv(MOCK_USER_RATINGS_FILE)\n",
    "      \n",
    "      # (restaurant_id, user_idë¥¼ ë¬¸ìì—´ë¡œ í†µì¼)\n",
    "      if 'restaurant_id' in df_all_user_ratings.columns:\n",
    "        df_all_user_ratings['restaurant_id'] = df_all_user_ratings['restaurant_id'].apply(str)\n",
    "      else:\n",
    "        print(\"[ê²½ê³ ] 'restaurant_id' ì»¬ëŸ¼ì´ 500ëª… í‰ê°€ íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        raise KeyError(\"'restaurant_id' ì»¬ëŸ¼ ëˆ„ë½\")\n",
    "      \n",
    "      if 'user_id' in df_all_user_ratings.columns:\n",
    "        df_all_user_ratings['user_id'] = df_all_user_ratings['user_id'].apply(str)\n",
    "      else:\n",
    "        print(\"[ê²½ê³ ] 'user_id' ì»¬ëŸ¼ì´ 500ëª… í‰ê°€ íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        raise KeyError(\"'user_id' ì»¬ëŸ¼ ëˆ„ë½\")\n",
    "\n",
    "      print(\"  > ì‹ë‹¹(restaurant_id)ë³„ 'ì¶”ì²œ', 'ë¯¸ì¶”ì²œ' ì¹´ìš´íŠ¸ ì§‘ê³„ ì¤‘...\")\n",
    "      \n",
    "      valid_ratings = df_all_user_ratings[\n",
    "        df_all_user_ratings['ì‚¬ìš©ìí‰ê°€'].isin(['ì¶”ì²œ', 'ë¯¸ì¶”ì²œ'])\n",
    "      ]\n",
    "      \n",
    "      ratings_crosstab = pd.crosstab(\n",
    "        valid_ratings['restaurant_id'], \n",
    "        valid_ratings['ì‚¬ìš©ìí‰ê°€']\n",
    "      )\n",
    "      \n",
    "      if 'ì¶”ì²œ' not in ratings_crosstab.columns:\n",
    "        ratings_crosstab['ì¶”ì²œ'] = 0\n",
    "      if 'ë¯¸ì¶”ì²œ' not in ratings_crosstab.columns:\n",
    "        ratings_crosstab['ë¯¸ì¶”ì²œ'] = 0\n",
    "        \n",
    "      df_restaurant_ratings_summary = ratings_crosstab[['ì¶”ì²œ', 'ë¯¸ì¶”ì²œ']].reset_index()\n",
    "      \n",
    "      print(f\"  > 500ëª… í‰ê°€ ë°ì´í„° ì§‘ê³„ ì™„ë£Œ: {len(df_restaurant_ratings_summary)}ê°œ ì‹ë‹¹\")\n",
    "      ratings_loaded_ok = True\n",
    "      \n",
    "    except FileNotFoundError:\n",
    "      print(f\"[ê²½ê³ ] {MOCK_USER_RATINGS_FILE} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (í‰ê°€ ì¹´ìš´íŠ¸ ê¸°ëŠ¥ ë¹„í™œì„±í™”)\")\n",
    "      ratings_loaded_ok = True \n",
    "    except Exception as e:\n",
    "      print(f\"[ê²½ê³ ] 500ëª… í‰ê°€ ë°ì´í„° ë¡œë“œ/ì§‘ê³„ ì¤‘ ì˜¤ë¥˜: {e} (í‰ê°€ ì¹´ìš´íŠ¸ ê¸°ëŠ¥ ë¹„í™œì„±í™”)\")\n",
    "      ratings_loaded_ok = True\n",
    "\n",
    "  # (3) ë²¡í„° DB ë¡œë“œ (collection, profile_collection ì „ì—­ ë³€ìˆ˜ ìƒì„±)\n",
    "  # (â˜…â˜…â˜… ìˆ˜ì •: 7ë²ˆ ì…€ í•¨ìˆ˜ê°€ 2ê°œ ì¸ìë¥¼ ë°›ë„ë¡ ë³€ê²½ë¨ â˜…â˜…â˜…)\n",
    "  db_load_ok = build_vector_db(\n",
    "    RESTAURANT_DB_FILE,    # (ê°€ê²Œ DB ê²½ë¡œ)\n",
    "    PROFILE_DB_FILE,       # (í”„ë¡œí•„ DB ê²½ë¡œ)\n",
    "    clear_db=CLEAR_DB_AND_REBUILD\n",
    "  )\n",
    "  \n",
    "  # (â˜…â˜…â˜… ìµœì¢… ê²€ì‚¬: data_loaded_okì™€ db_load_okë§Œ í•„ìˆ˜)\n",
    "  if not data_loaded_ok or not db_load_ok:\n",
    "     print(\"[ì¹˜ëª…ì  ì˜¤ë¥˜] DB ë˜ëŠ” ê°€ê²Œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. Gradioë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "     exit() \n",
    "    \n",
    "except Exception as e:\n",
    "  print(f\"[ì¹˜ëª…ì  ì˜¤ë¥˜] DB ë¡œë“œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}\")\n",
    "  exit()\n",
    "\n",
    "\n",
    "# --- 9. Gradio UI ì¸í„°í˜ì´ìŠ¤ ë¹Œë“œ ---\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "  gr.Markdown(\"# ğŸ¤– gpt-4.1-mini ê¸°ë°˜ ìì—°ì–´ ì„œë² ì´ ì±—ë´‡ (ì¶”ì²œ ê¸°ëŠ¥)\")\n",
    "  gr.Markdown(\"AIê°€ 13ê°€ì§€ í”„ë¡œí•„ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³ , ì™„ë£Œë˜ë©´ ë§ì¶¤ ì‹ë‹¹ì„ ì¶”ì²œí•©ë‹ˆë‹¤.\")\n",
    "  \n",
    "  with gr.Row():\n",
    "    with gr.Column(scale=2):\n",
    "      # 1. ë³´ì´ì§€ ì•ŠëŠ” ìƒíƒœ(State) ë³€ìˆ˜ ì„ ì–¸\n",
    "      llm_history_state = gr.State(value=[]) \n",
    "      profile_state = gr.State(value=PROFILE_TEMPLATE.copy())\n",
    "      is_completed_state = gr.State(value=False)\n",
    "\n",
    "      # 2. ì±„íŒ…ì°½ ì»´í¬ë„ŒíŠ¸\n",
    "      chatbot = gr.Chatbot(\n",
    "        label=\"ì„œë² ì´ ì±—ë´‡\", \n",
    "        height=700, \n",
    "        show_copy_button=True,\n",
    "        type='messages' \n",
    "      )\n",
    "      \n",
    "      # 3. ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸ë°•ìŠ¤\n",
    "      msg_textbox = gr.Textbox(\n",
    "        label=\"ë‹µë³€ ì…ë ¥\", \n",
    "        placeholder=\"ì—¬ê¸°ì— ë‹µë³€ì„ ì…ë ¥í•˜ê³  Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...\"\n",
    "      )\n",
    "    \n",
    "    with gr.Column(scale=1):\n",
    "      # (ì‹ ê·œ: ì •ì  ì¶”ì²œ ê²°ê³¼ ì˜ì—­)\n",
    "      gr.Markdown(\"### ğŸŒŸ ë§ì¶¤ ì¶”ì²œ ê²°ê³¼\")\n",
    "      recommendation_output = gr.Markdown(\n",
    "        label=\"ì¶”ì²œ ê²°ê³¼\",\n",
    "        value=\"...í”„ë¡œí•„ ì„¤ë¬¸ì´ ì™„ë£Œë˜ë©´ ì—¬ê¸°ì— ì¶”ì²œ ê²°ê³¼ê°€ í‘œì‹œë©ë‹ˆë‹¤...\",\n",
    "        visible=False\n",
    "      )\n",
    "\n",
    "  # 4. ì•±ì´ ì²˜ìŒ ë¡œë“œë  ë•Œ ì‹¤í–‰í•  í•¨ìˆ˜ ì—°ê²°\n",
    "  demo.load(\n",
    "    fn=start_chat,\n",
    "    inputs=None,\n",
    "    outputs=[chatbot, llm_history_state, profile_state, is_completed_state]\n",
    "  )\n",
    "  \n",
    "  # 5. ì‚¬ìš©ìê°€ í…ìŠ¤íŠ¸ë°•ìŠ¤ì—ì„œ Enter(submit)ë¥¼ ëˆ„ë¥¼ ë•Œ ì‹¤í–‰í•  í•¨ìˆ˜ ì—°ê²°\n",
    "  msg_textbox.submit(\n",
    "    fn=chat_survey,\n",
    "    inputs=[msg_textbox, chatbot, llm_history_state, profile_state, is_completed_state],\n",
    "    outputs=[\n",
    "      chatbot, llm_history_state, profile_state, \n",
    "      is_completed_state, recommendation_output\n",
    "    ]\n",
    "  )\n",
    "  \n",
    "  # 6. Enter ëˆ„ë¥¸ í›„ í…ìŠ¤íŠ¸ë°•ìŠ¤ ë¹„ìš°ê¸°\n",
    "  msg_textbox.submit(lambda: \"\", inputs=None, outputs=msg_textbox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradio ì•±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- í”„ë¡œí•„ ì™„ì„±! ìš”ì•½ ë° í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤. ---\n",
      "\n",
      "--- 3ë‹¨ê³„: RAG + ì ìˆ˜ì œ(Scoring) ê²€ìƒ‰ ì‹œì‘ ---\n",
      "  > [RAG] LLMì„ í˜¸ì¶œí•˜ì—¬ 'ë¶„ìœ„ê¸°/ì„±í–¥' ì¿¼ë¦¬ë¥¼ ì¬ì‘ì„±í•©ë‹ˆë‹¤...\n",
      "í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì: Chloe (live_user)\n",
      "RAG ì¿¼ë¦¬ (ì¬ì‘ì„±ë¨): 'ì—°ì¸ê³¼ í•¨ê»˜ ë¹ ë¥´ê³  í¸ì•ˆí•˜ê²Œ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ë¶„ìœ„ê¸°ì˜ ì¸ê¸° ë§›ì§‘'\n",
      "DB 1ì°¨ í•„í„° (ChromaDB): {'$and': [{'budget_range': 'ì¤‘'}, {'spicy_available': 'O'}, {'vegetarian_options': 'X'}, {'high_level_category': 'ìƒê´€ì—†ìŒ'}]}\n",
      "Python 2ì°¨ í•„í„° (ì ìˆ˜ ê³„ì‚°ìš©): {'main_ingredients_list': ['í•´ì‚°ë¬¼'], 'suitable_for': ['ì—°ì¸']}\n",
      "\n",
      "--- 1ë‹¨ê³„: RAG + 1ì°¨ í•„í„° ê²€ìƒ‰ | Top 50ê°œ ì°¾ê¸° ---\n",
      "--- 1ì°¨ ê²€ìƒ‰ ì™„ë£Œ: 0ê°œ í›„ë³´ ë°˜í™˜ ---\n",
      "  > [í•„í„° ì™„í™”] 1ì°¨ í•„í„° ê²°ê³¼ 0ê±´. RAG-Only(í•„í„° ì—†ìŒ)ë¡œ ì¬ì‹œë„...\n",
      "  > RAG-Only ê²€ìƒ‰ ì™„ë£Œ: 50ê°œ í›„ë³´ ë°˜í™˜\n",
      "\n",
      "--- 2ë‹¨ê³„: ì ìˆ˜(Scoring) ê³„ì‚° ì‹œì‘ (1ì°¨ í›„ë³´ ëŒ€ìƒ) ---\n",
      "\n",
      "--- 3ë‹¨ê³„: ìµœì¢… RAG + ì ìˆ˜ì œ ë­í‚¹ ê²°ê³¼ (ê¸°ë³¸ ì¶”ì²œ) ---\n",
      "\n",
      "--- 4ë‹¨ê³„: ìœ ì‚¬ ì‚¬ìš©ì ê¸°ë°˜ ì¶”ê°€ ì¶”ì²œ ê²€ìƒ‰ ---\n",
      "[ìœ ì‚¬ ì¶”ì²œ] ì°¾ì€ ìœ ì‚¬ ì‚¬ìš©ì: ['user_0450']\n",
      "[ìœ ì‚¬ ì¶”ì²œ] ì¶”ê°€í•  ì‹ë‹¹: ['31306', '26271']\n",
      "\n",
      "======================================================================\n",
      " [í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìš© ë°ì´í„° ìƒì„± ì™„ë£Œ] \n",
      "\n",
      "[1] name:\n",
      "Chloe\n",
      "\n",
      "[2] rag_query_text:\n",
      "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” Chloeì…ë‹ˆë‹¤. ì €ëŠ” 30ëŒ€ ë¯¸êµ­ ì—¬ì„±ì´ê³ , ì—°ì¸ê³¼ í•¨ê»˜í•˜ëŠ” ì—¬í–‰ì„ ì •ë§ ì¢‹ì•„í•´ìš”. ì €í¬ëŠ” ë‘˜ì´ì„œ ë‹¤ë‹ˆëŠ” í¸ì´ê³ , ê¸°ë‹¤ë¦¬ëŠ” ê±¸ ë³„ë¡œ ì¢‹ì•„í•˜ì§€ ì•Šì•„ì„œ ë¹ ë¥´ê²Œ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ê³³ì„ ì„ í˜¸í•´ìš”. ì˜ˆì‚°ì€ ì ë‹¹í•œ í¸ì´ë¼ ê°€ì„±ë¹„ë„ ì±™ê¸°ë©´ì„œ ë§›ìˆëŠ” ìŒì‹ì„ ì¦ê¸°ê³  ì‹¶ê³ ìš”. ë§¤ìš´ ìŒì‹ë„ ì˜ ë¨¹ëŠ” í¸ì´ë¼ ì•½ê°„ ë§¤ì½¤í•œ ë§›ë„ ì¢‹ì•„í•´ìš”. ì±„ì‹ì£¼ì˜ìëŠ” ì•„ë‹ˆê³ , íŠ¹íˆ í•´ì‚°ë¬¼ì„ ì¢‹ì•„í•´ì„œ í•´ì‚°ë¬¼ì´ ë“¤ì–´ê°„ ë©”ë‰´ë¼ë©´ ì–¸ì œë“  í™˜ì˜ì…ë‹ˆë‹¤. ìŒì‹ ì¢…ë¥˜ëŠ” íŠ¹ë³„íˆ ê°€ë¦¬ì§€ ì•Šìœ¼ë‹ˆ ë‹¤ì–‘í•œ ë§›ì„ ê²½í—˜í•˜ëŠ” ê±¸ ì¦ê¸°ê³  ìˆë‹µë‹ˆë‹¤.\n",
      "\n",
      "[3] filter_metadata_json:\n",
      "{\"budget_range\": \"ì¤‘\", \"spicy_available\": \"O\", \"vegetarian_options\": \"X\", \"main_ingredients_list\": \"í•´ì‚°ë¬¼\", \"suitable_for\": \"ì—°ì¸\", \"food_category\": \"ìƒê´€ì—†ìŒ\"}\n",
      "======================================================================\n",
      "\n",
      "{\n",
      "  \"name\": \"Chloe\",\n",
      "  \"age\": \"30ëŒ€\",\n",
      "  \"gender\": \"ì—¬\",\n",
      "  \"nationality\": \"ë¯¸êµ­\",\n",
      "  \"travel_type\": \"ì—°ì¸\",\n",
      "  \"party_size\": 2,\n",
      "  \"can_wait\": \"X\",\n",
      "  \"budget\": \"ì¤‘\",\n",
      "  \"spicy_ok\": \"O\",\n",
      "  \"is_vegetarian\": \"X\",\n",
      "  \"avoid_ingredients\": \"ì—†ìŒ\",\n",
      "  \"like_ingredients\": \"í•´ì‚°ë¬¼\",\n",
      "  \"food_category\": \"ìƒê´€ì—†ìŒ\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- 10. ì•± ì‹¤í–‰ ---\n",
    "if __name__ == \"__main__\":\n",
    "  if 'collection' not in globals() or 'df_restaurants' not in globals() or 'df_menus' not in globals():\n",
    "     print(\"[ì¢…ë£Œ] DBê°€ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "  else:\n",
    "     print(\"Gradio ì•±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "     demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
