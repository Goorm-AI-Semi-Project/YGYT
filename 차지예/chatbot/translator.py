"""
translator.py
- NLLB-200 ê¸°ë°˜ ë‹¤êµ­ì–´ â†” í•œêµ­ì–´ ë²ˆì—­ ëª¨ë“ˆ
- ê¸´ í…ìŠ¤íŠ¸ëŠ” ìë™ìœ¼ë¡œ ì˜ë¼ì„œ ì—¬ëŸ¬ ë²ˆ ë²ˆì—­ í›„ ì´ì–´ë¶™ì„
"""

from functools import lru_cache
from typing import Literal, List

from langdetect import detect
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

# ì‚¬ìš©í•  HuggingFace ëª¨ë¸
NLLB_MODEL_NAME = "facebook/nllb-200-distilled-600M"

# ìš°ë¦¬ ì„œë¹„ìŠ¤ì—ì„œ ì“¸ ì–¸ì–´ ì½”ë“œ â†’ NLLB ì–¸ì–´ ì½”ë“œ ë§¤í•‘
LANG_CODE_MAP = {
    "ko": "kor_Hang",
    "en": "eng_Latn",
    "ja": "jpn_Jpan",
    "zh": "zho_Hans",  # ì¤‘êµ­ì–´ ê°„ì²´
}

UserLang = Literal["ko", "en", "ja", "zh"]


@lru_cache(maxsize=1)
def get_nllb_pipeline():
    """
    NLLB íŒŒì´í”„ë¼ì¸ì„ lazy ë¡œë”© + ìºì‹œ (í”„ë¡œì„¸ìŠ¤ë‹¹ í•œ ë²ˆë§Œ ë¡œë“œ)
    max_lengthëŠ” ëª¨ë¸ ê¸°ë³¸ê°’ì„ ì“°ê³ , ëŒ€ì‹  ê¸´ í…ìŠ¤íŠ¸ëŠ” ìš°ë¦¬ê°€ ì§ì ‘ ì˜ë¼ì„œ ë³´ëƒ„.
    """
    tokenizer = AutoTokenizer.from_pretrained(NLLB_MODEL_NAME)
    model = AutoModelForSeq2SeqLM.from_pretrained(NLLB_MODEL_NAME)
    pipe = pipeline(
        "translation",
        model=model,
        tokenizer=tokenizer,
        # max_lengthë¥¼ êµ³ì´ ì‘ê²Œ ì§€ì •í•˜ì§€ ì•Šê³ , ëª¨ë¸ ê¸°ë³¸ê°’ ì‚¬ìš©
        # ê¸´ í…ìŠ¤íŠ¸ëŠ” ì•„ë˜ _split_long_text ì—ì„œ ì˜ë¼ì„œ ì²˜ë¦¬
    )
    return pipe


def _split_long_text(text: str, max_chunk_chars: int = 400) -> List[str]:
    """
    ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì—¬ëŸ¬ chunkë¡œ ë‚˜ëˆ„ê¸° (ë¬¸ë‹¨/ë¬¸ì¥ ê¸°ì¤€)
    - ë¬¸ì ê¸°ì¤€ìœ¼ë¡œ ìë¥´ì§€ë§Œ, 400ì ì •ë„ë©´ ëŒ€ë¶€ë¶„ 512 í† í° ì•„ë˜ë¡œ ë“¤ì–´ê°
    """
    text = text or ""
    if len(text) <= max_chunk_chars:
        return [text]

    chunks: List[str] = []
    current = ""

    def flush():
        nonlocal current
        if current.strip():
            chunks.append(current.strip())
        current = ""

    # 1ì°¨: ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
    paragraphs = text.split("\n\n")
    for para in paragraphs:
        para = para.strip()
        if not para:
            continue

        # ë¬¸ë‹¨ì´ ë„ˆë¬´ ê¸¸ë©´ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë” ìª¼ê° ë‹¤
        if len(para) > max_chunk_chars:
            sentence_buf = ""
            for ch in para:
                sentence_buf += ch
                # ë¬¸ì¥ ëìœ¼ë¡œ ë³¼ ìˆ˜ ìˆëŠ” ë¬¸ìë“¤
                if ch in ".!?ã€‚ï¼Ÿï¼\n" and len(sentence_buf) >= max_chunk_chars:
                    chunks.append(sentence_buf.strip())
                    sentence_buf = ""
            if sentence_buf.strip():
                chunks.append(sentence_buf.strip())
        else:
            # í˜„ì¬ ë²„í¼ì— ë”í•´ë„ ë˜ë©´ ë”í•˜ê³ , ì•„ë‹ˆë©´ flush í›„ ìƒˆë¡œ ì‹œì‘
            if len(current) + len(para) + 2 > max_chunk_chars:
                flush()
            current += para + "\n\n"

    flush()

    # í˜¹ì‹œë¼ë„ ê³µë°± chunkê°€ ì„ì—¬ ìˆìœ¼ë©´ ì œê±°
    return [c for c in chunks if c.strip()]


def translate_text(
    text: str,
    src_lang: UserLang,
    tgt_lang: UserLang,
) -> str:
    """
    ko/en/ja/zh ì‚¬ì´ ë²ˆì—­.
    - src_lang == tgt_lang ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    - ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì—¬ëŸ¬ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ ì„œ ë²ˆì—­ í›„ ë‹¤ì‹œ ì´ì–´ë¶™ì„
    """
    if not text:
        return text

    if src_lang == tgt_lang:
        return text

    if src_lang not in LANG_CODE_MAP or tgt_lang not in LANG_CODE_MAP:
        # ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ
        return text

    pipe = get_nllb_pipeline()
    src = LANG_CODE_MAP[src_lang]
    tgt = LANG_CODE_MAP[tgt_lang]

    # ğŸ”¹ ê¸´ í…ìŠ¤íŠ¸ ë¶„í• 
    chunks = _split_long_text(text, max_chunk_chars=400)
    translated_chunks: List[str] = []

    try:
        # í•œ ë²ˆì— ì—¬ëŸ¬ chunkë¥¼ ë„£ì–´ì„œ batch ë²ˆì—­
        outputs = pipe(chunks, src_lang=src, tgt_lang=tgt)
        for out in outputs:
            translated_chunks.append(out["translation_text"])
    except Exception as e:
        print(f"[translator] ë²ˆì—­ ì˜¤ë¥˜: {e}")
        # ì—ëŸ¬ê°€ ë‚˜ë©´ ì•ˆì „í•˜ê²Œ ì›ë¬¸ì„ ëŒë ¤ì¤€ë‹¤
        return text

    return "\n\n".join(translated_chunks)


def detect_lang_simple(text: str) -> UserLang:
    """
    ìë™ ê°ì§€ê°€ í•„ìš”í•  ë•Œ ì“¸ ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ ê°ì§€ê¸°.
    (ì§€ê¸ˆ êµ¬ì¡°ì—ì„œëŠ” ë³´í†µ 'ì„ íƒëœ ì–¸ì–´'ë¥¼ ì“°ê¸° ë•Œë¬¸ì— ìì£¼ ì“°ì´ì§„ ì•ŠìŒ)
    """
    if not text or len(text.strip()) == 0:
        return "ko"

    try:
        code = detect(text)
    except Exception:
        return "en"

    if code.startswith("zh"):
        return "zh"
    if code == "ja":
        return "ja"
    if code == "ko":
        return "ko"
    return "en"
