{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "826e88e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] cards: 32\n",
      "{'name': '다이닝마', 'address': '서울특별시 강남구 언주로152길 8 (신사동) 유일빌딩 2층'}\n",
      "{'name': '쵸이닷', 'address': '서울특별시 강남구 도산대로 457 (청담동) 앙스돔빌딩 3층'}\n",
      "{'name': '비스트로드욘트빌', 'address': '서울특별시 강남구 선릉로158길 13-7 (청담동) 이안빌딩 1층'}\n",
      "{'name': '밍글스', 'address': '서울특별시 강남구 도산대로67길 19 (청담동) 힐탑빌딩 2층'}\n",
      "{'name': '라미띠에', 'address': '서울특별시 강남구 도산대로67길 30 (청담동) 2층'}\n",
      "{'name': '권숙수', 'address': '서울특별시 강남구 압구정로80길 37 (청담동) 이에스빌딩 4층'}\n",
      "{'name': '레스쁘아', 'address': '서울특별시 강남구 도산대로56길 10 (청담동) 2, 3층'}\n",
      "{'name': '벽제갈비더청담', 'address': '서울특별시 강남구 도산대로81길 25 (청담동) 조은빌딩 1층'}\n",
      "{'name': '홍보각', 'address': '서울특별시 강남구 봉은사로 130 (역삼동) 노보텔앰배서더강남서울 LL층'}\n",
      "{'name': '낙원', 'address': '서울특별시 강서구 방화대로 94 (외발산동) 메이필드호텔 1층'}\n",
      "{'name': '삼원가든', 'address': '서울특별시 강남구 언주로 835 (신사동)'}\n",
      "{'name': '강민철레스토랑', 'address': '서울특별시 강남구 도산대로63길 18 (청담동) 청담빌딩 5층'}\n",
      "{'name': '파씨오네', 'address': '서울특별시 강남구 언주로164길 39 (신사동) 2층'}\n",
      "{'name': '세븐스도어', 'address': '서울특별시 강남구 학동로97길 41 (청담동) 리유빌딩 4층'}\n",
      "{'name': '봉래헌', 'address': '서울특별시 강서구 방화대로 94 (외발산동) 메이필드호텔'}\n",
      "{'name': '미피아체', 'address': '서울특별시 강남구 도산대로70길 34 (청담동)'}\n",
      "{'name': '정식당', 'address': '서울특별시 강남구 선릉로158길 11 (청담동)'}\n",
      "{'name': '스와니예', 'address': '서울특별시 강남구 강남대로 652 (신사동) 신사스퀘어 2층'}\n",
      "{'name': '알라프리마', 'address': '서울특별시 강남구 학동로17길 13 (논현동)'}\n",
      "{'name': '무오키', 'address': '서울특별시 강남구 학동로55길 12-12 (청담동)'}\n",
      "{'name': '고료리켄', 'address': '서울특별시 강남구 언주로152길 15-3 (신사동) 2층'}\n",
      "{'name': '미토우', 'address': '서울특별시 강남구 도산대로70길 24 (청담동)'}\n",
      "{'name': '코자차', 'address': '서울특별시 강남구 학동로97길 17 (청담동)'}\n",
      "{'name': '이타닉가든', 'address': '서울특별시 강남구 테헤란로 231 (역삼동) 조선팰리스강남 36층'}\n",
      "{'name': '솔밤', 'address': '서울특별시 강남구 학동로 231 (논현동) 논현백영센터 2층'}\n",
      "{'name': '듀자미', 'address': '서울특별시 강남구 도산대로11길 28 (신사동)'}\n",
      "{'name': '브레드피트', 'address': '서울특별시 영등포구 국제금융로 86 (여의도동) 롯데캐슬아이비 지하1층 129호'}\n",
      "{'name': '오뗄두스', 'address': '서울특별시 서초구 서래로10길 9 (반포동) 서래빌딩'}\n",
      "{'name': '버드나무집', 'address': '서울특별시 서초구 효령로 434 (서초동)'}\n",
      "{'name': '미우야', 'address': '서울특별시 서초구 강남대로23길 90 (양재동) 기정빌딩 1층'}\n",
      "{'name': '한성칼국수', 'address': '서울특별시 강남구 언주로148길 14 (논현동) 청호상가빌딩 가동 2층'}\n",
      "{'name': '새벽집', 'address': '서울특별시 강남구 도산대로101길 6 (청담동)'}\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "URL = \"https://www.bluer.co.kr/search?query=&zone1=%EC%84%9C%EC%9A%B8%20%EA%B0%95%EB%82%A8\"  # 실제 목록 URL로 교체\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    return \" \".join(s.split()) if s else s\n",
    "\n",
    "def wait_cards_stable(driver, css, min_wait=6, check_every=0.6):\n",
    "    \"\"\"카드 개수가 증가가 멈출 때까지 잠깐 대기\"\"\"\n",
    "    end = time.time() + min_wait\n",
    "    prev = -1\n",
    "    while time.time() < end:\n",
    "        cur = len(driver.find_elements(By.CSS_SELECTOR, css))\n",
    "        if cur == prev and cur > 0:\n",
    "            break\n",
    "        prev = cur\n",
    "        time.sleep(check_every)\n",
    "    return prev\n",
    "\n",
    "opts = Options()\n",
    "opts.add_argument(\"--headless=new\")        # 안 보이면 주석 처리 후 확인\n",
    "opts.add_argument(\"--window-size=1280,2000\")\n",
    "opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "driver = webdriver.Chrome(options=opts)\n",
    "driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
    "    \"source\": \"Object.defineProperty(navigator,'webdriver',{get:()=>undefined});\"\n",
    "})\n",
    "\n",
    "try:\n",
    "    driver.get(URL)\n",
    "\n",
    "    CARD_SEL = \"li.rl-col.restaurant-thumb-item\"\n",
    "\n",
    "    # 1) 기본 페이지에서 먼저 탐색\n",
    "    try:\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, CARD_SEL))\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    count = len(driver.find_elements(By.CSS_SELECTOR, CARD_SEL))\n",
    "\n",
    "    # 2) 카드가 안 보이면 iframe 내부일 수 있으니 프레임 자동 전환 시도\n",
    "    if count == 0:\n",
    "        iframes = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "        for f in iframes:\n",
    "            driver.switch_to.default_content()\n",
    "            driver.switch_to.frame(f)\n",
    "            if len(driver.find_elements(By.CSS_SELECTOR, CARD_SEL)) > 0:\n",
    "                break\n",
    "        # 프레임 전환 후에도 한 번 더 대기\n",
    "        if len(driver.find_elements(By.CSS_SELECTOR, CARD_SEL)) == 0:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, CARD_SEL))\n",
    "            )\n",
    "\n",
    "    # 3) 개수 안정화까지 잠깐 기다림(동적 템플릿이 한번에 쏟아질 때 대비)\n",
    "    stable_count = wait_cards_stable(driver, CARD_SEL)\n",
    "    print(f\"[DEBUG] cards: {stable_count}\")\n",
    "\n",
    "    cards = driver.find_elements(By.CSS_SELECTOR, CARD_SEL)\n",
    "    results = []\n",
    "\n",
    "    for idx, card in enumerate(cards, 1):\n",
    "        # (A) 직접 셀렉터로 시도\n",
    "        name = None\n",
    "        address = None\n",
    "        try:\n",
    "            h3 = card.find_element(By.CSS_SELECTOR, \".header-title h3\")\n",
    "            name = clean_text(h3.text) or clean_text(h3.get_attribute(\"innerText\"))\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            addr_el = card.find_element(By.CSS_SELECTOR, \".thumb-caption .info .info-item .content-info.juso-info\")\n",
    "            address = clean_text(addr_el.text) or clean_text(addr_el.get_attribute(\"innerText\"))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # (B) 그래도 못 찾으면 innerHTML을 BS로 파싱해서 재시도(사이트에 따라 .text가 빈 경우가 있음)\n",
    "        if not name or not address:\n",
    "            html = card.get_attribute(\"innerHTML\")\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            if not name:\n",
    "                name_el = soup.select_one(\".header-title h3, .clearfix > h3\")\n",
    "                name = clean_text(name_el.get_text()) if name_el else None\n",
    "            if not address:\n",
    "                addr_el = soup.select_one(\".content-info.juso-info\")\n",
    "                address = clean_text(addr_el.get_text()) if addr_el else None\n",
    "\n",
    "        if name or address:\n",
    "            results.append({\"name\": name, \"address\": address})\n",
    "\n",
    "    # 출력\n",
    "    for r in results:\n",
    "        print(r)\n",
    "\n",
    "    if not results:\n",
    "        print(\"\\n[HINT] 결과가 없으면 다음을 확인하세요:\")\n",
    "        print(\"- headless 모드 해제하고 실제 카드가 렌더되는지 확인\")\n",
    "        print(\"- 선택자: 카드(li.rl-col.restaurant-thumb-item) / 이름(.header-title h3) / 주소(.content-info.juso-info)\")\n",
    "        print(\"- 리스트가 iframe 안에 있는지(코드에 자동 전환 로직 포함)\")\n",
    "        print(\"- 로그인/지역 선택 등 접근 게이트 유무\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9f9f122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\mynote\\anaconda3\\lib\\site-packages (4.36.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\mynote\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio<1.0,>=0.30.0 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from selenium) (0.31.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from selenium) (2025.10.5)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.14.0 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (24.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio<1.0,>=0.30.0->selenium) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\mynote\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d63a76dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAGE 1] rows: 32\n",
      "saved: bluer_gangnam.csv (32 rows)\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "URL = \"https://www.bluer.co.kr/search?query=&zone1=%EC%84%9C%EC%9A%B8%20%EA%B0%95%EB%82%A8\"\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    return \" \".join(s.split()) if s else s\n",
    "\n",
    "def get_cards_on_page(driver):\n",
    "    \"\"\"현재 페이지의 카드에서 필요한 정보 추출\"\"\"\n",
    "    rows = []\n",
    "\n",
    "    WebDriverWait(driver, 15).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"li.rl-col.restaurant-thumb-item\"))\n",
    "    )\n",
    "    time.sleep(0.6)  # 렌더 안정화\n",
    "\n",
    "    cards = driver.find_elements(By.CSS_SELECTOR, \"li.rl-col.restaurant-thumb-item\")\n",
    "    for card in cards:\n",
    "        name = address = food_type = label = None\n",
    "        ribbon_count = 0\n",
    "\n",
    "        # 가게명\n",
    "        try:\n",
    "            h3 = card.find_element(By.CSS_SELECTOR, \".header-title h3\")\n",
    "            name = clean_text(h3.text) or clean_text(h3.get_attribute(\"innerText\"))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # 주소\n",
    "        try:\n",
    "            addr_el = card.find_element(By.CSS_SELECTOR, \".thumb-caption .info .info-item .content-info.juso-info\")\n",
    "            address = clean_text(addr_el.text) or clean_text(addr_el.get_attribute(\"innerText\"))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # 카테고리(음식 종류)\n",
    "        try:\n",
    "            ft = card.find_element(By.CSS_SELECTOR, \".header-status .foodtype li\")\n",
    "            food_type = clean_text(ft.text)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # 리본 개수(이미지 개수)\n",
    "        try:\n",
    "            ribbon_imgs = card.find_elements(By.CSS_SELECTOR, \".header-title .ribbons .img-ribbon\")\n",
    "            ribbon_count = len(ribbon_imgs)\n",
    "        except:\n",
    "            ribbon_count = 0\n",
    "\n",
    "        # 라벨(예: '서울 2025 선정') — 여러 개면 공백으로 연결\n",
    "        try:\n",
    "            label_els = card.find_elements(By.CSS_SELECTOR, \".header-title .header-labels li\")\n",
    "            labels = [clean_text(el.text) for el in label_els if clean_text(el.text)]\n",
    "            label = \" \".join(labels) if labels else None\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # .text가 비는 특수 케이스 대비: innerHTML 보조 파싱\n",
    "        if not (name and address and (food_type or label or ribbon_count >= 0)):\n",
    "            html = card.get_attribute(\"innerHTML\")\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            if not name:\n",
    "                el = soup.select_one(\".header-title h3, .clearfix > h3\")\n",
    "                name = clean_text(el.get_text()) if el else name\n",
    "            if not address:\n",
    "                el = soup.select_one(\".content-info.juso-info\")\n",
    "                address = clean_text(el.get_text()) if el else address\n",
    "            if not food_type:\n",
    "                el = soup.select_one(\".header-status .foodtype li\")\n",
    "                food_type = clean_text(el.get_text()) if el else food_type\n",
    "            if ribbon_count == 0:\n",
    "                ribbon_count = len(soup.select(\".header-title .ribbons .img-ribbon\"))\n",
    "            if not label:\n",
    "                labels = [clean_text(li.get_text()) for li in soup.select(\".header-title .header-labels li\")]\n",
    "                label = \" \".join(labels) if labels else label\n",
    "\n",
    "        if name or address:\n",
    "            rows.append({\n",
    "                \"가게\": name,\n",
    "                \"주소\": address,\n",
    "                \"카테고리\": food_type,\n",
    "                \"리본개수\": ribbon_count,\n",
    "                \"라벨\": label,\n",
    "            })\n",
    "\n",
    "    return rows\n",
    "\n",
    "def click_next_page(driver):\n",
    "    \"\"\"다음 페이지로 이동. 성공시 True, 없으면 False.\"\"\"\n",
    "    candidates = [\n",
    "        (By.CSS_SELECTOR, 'a[rel=\"next\"]'),\n",
    "        (By.XPATH, \"//a[contains(normalize-space(.), '다음')]\"),\n",
    "        (By.CSS_SELECTOR, \".pagination a.next, .paginate a.next, a.page-link.next\"),\n",
    "    ]\n",
    "    for by, sel in candidates:\n",
    "        try:\n",
    "            btns = driver.find_elements(by, sel)\n",
    "            for b in btns:\n",
    "                if b.is_displayed() and b.is_enabled():\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", b)\n",
    "                    time.sleep(0.2)\n",
    "                    b.click()\n",
    "                    return True\n",
    "        except:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "def crawl_and_save(url=URL, headless=False, delay_between_pages=0.8, out_csv=\"bluer_gangnam.csv\"):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--window-size=1400,2200\")\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        all_rows = []\n",
    "        page = 1\n",
    "        while True:\n",
    "            page_rows = get_cards_on_page(driver)\n",
    "            print(f\"[PAGE {page}] rows: {len(page_rows)}\")\n",
    "            all_rows.extend(page_rows)\n",
    "\n",
    "            if not click_next_page(driver):\n",
    "                break\n",
    "            page += 1\n",
    "            time.sleep(delay_between_pages)\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    # DataFrame 만들고 컬럼 순서 고정\n",
    "    df = pd.DataFrame(all_rows, columns=[\"가게\", \"주소\", \"카테고리\", \"리본개수\", \"라벨\"])\n",
    "    # 중복 제거(가게+주소 기준)\n",
    "    df = df.drop_duplicates(subset=[\"가게\", \"주소\"])\n",
    "\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"saved: {out_csv} ({len(df)} rows)\")\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_and_save(headless=False)  # 처음엔 False로 화면 확인 권장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb7438c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAGE 1] rows: 32\n",
      "saved: bluer_gangnam.csv (32 rows)\n"
     ]
    }
   ],
   "source": [
    "# pip install selenium beautifulsoup4 pandas\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "URL = \"https://www.bluer.co.kr/search?query=&zone1=%EC%84%9C%EC%9A%B8%20%EA%B0%95%EB%82%A8#restaurant-filter-bottom\"\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    return \" \".join(s.split()) if s else s\n",
    "\n",
    "def yesno(flag: bool) -> str:\n",
    "    return \"Y\" if flag else \"N\"\n",
    "\n",
    "def get_cards_on_page(driver):\n",
    "    rows = []\n",
    "    WebDriverWait(driver, 15).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"li.rl-col.restaurant-thumb-item\"))\n",
    "    )\n",
    "    time.sleep(0.6)\n",
    "\n",
    "    cards = driver.find_elements(By.CSS_SELECTOR, \"li.rl-col.restaurant-thumb-item\")\n",
    "    for card in cards:\n",
    "        name = address = None\n",
    "        ribbon_count = 0\n",
    "        food_types_joined = None\n",
    "        labels_joined = None\n",
    "        has_red = False\n",
    "        has_seoul2025 = False\n",
    "\n",
    "        # 가게명\n",
    "        try:\n",
    "            h3 = card.find_element(By.CSS_SELECTOR, \".header-title h3\")\n",
    "            name = clean_text(h3.text) or clean_text(h3.get_attribute(\"innerText\"))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # 주소\n",
    "        try:\n",
    "            addr_el = card.find_element(By.CSS_SELECTOR, \".thumb-caption .info .info-item .content-info.juso-info\")\n",
    "            address = clean_text(addr_el.text) or clean_text(addr_el.get_attribute(\"innerText\"))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # 카테고리(여러 개 → \", \"로 연결)\n",
    "        try:\n",
    "            ft_els = card.find_elements(By.CSS_SELECTOR, \".header-status .foodtype li\")\n",
    "            food_types = [clean_text(el.text) for el in ft_els if clean_text(el.text)]\n",
    "            food_types_joined = \", \".join(food_types) if food_types else None\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # 리본 개수(이미지 수)\n",
    "        try:\n",
    "            ribbon_imgs = card.find_elements(By.CSS_SELECTOR, \".header-title .ribbons .img-ribbon\")\n",
    "            ribbon_count = len(ribbon_imgs)\n",
    "        except:\n",
    "            ribbon_count = 0\n",
    "\n",
    "        # 라벨 수집 + 개별 플래그(레드리본 선정 / 서울 2025 선정)\n",
    "        try:\n",
    "            label_els = card.find_elements(By.CSS_SELECTOR, \".header-title .header-labels li\")\n",
    "            labels = [clean_text(el.text) for el in label_els if clean_text(el.text)]\n",
    "            labels_joined = \" \".join(labels) if labels else None\n",
    "            if labels:\n",
    "                # 개별 라벨 컬럼 플래그 설정\n",
    "                has_red = any(\"레드리본 선정\" in t for t in labels)\n",
    "                has_seoul2025 = any(\"서울 2025 선정\" in t for t in labels)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # 보조 파싱(혹시 text가 비는 케이스)\n",
    "        if not (name and address):\n",
    "            html = card.get_attribute(\"innerHTML\")\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            if not name:\n",
    "                el = soup.select_one(\".header-title h3, .clearfix > h3\")\n",
    "                name = clean_text(el.get_text()) if el else name\n",
    "            if not address:\n",
    "                el = soup.select_one(\".content-info.juso-info\")\n",
    "                address = clean_text(el.get_text()) if el else address\n",
    "            if food_types_joined is None:\n",
    "                fts = [clean_text(li.get_text()) for li in soup.select(\".header-status .foodtype li\") if clean_text(li.get_text())]\n",
    "                food_types_joined = \", \".join(fts) if fts else None\n",
    "            if ribbon_count == 0:\n",
    "                ribbon_count = len(soup.select(\".header-title .ribbons .img-ribbon\"))\n",
    "            if labels_joined is None:\n",
    "                labs = [clean_text(li.get_text()) for li in soup.select(\".header-title .header-labels li\")]\n",
    "                labels_joined = \" \".join(labs) if labs else None\n",
    "                has_red = any(\"레드리본 선정\" in t for t in labs)\n",
    "                has_seoul2025 = any(\"서울 2025 선정\" in t for t in labs)\n",
    "\n",
    "        if name or address:\n",
    "            rows.append({\n",
    "                \"가게\": name,\n",
    "                \"주소\": address,\n",
    "                \"카테고리\": food_types_joined,\n",
    "                \"리본개수\": ribbon_count,\n",
    "                \"레드리본 선정\": yesno(has_red),\n",
    "                \"서울 2025 선정\": yesno(has_seoul2025),\n",
    "                \"라벨\": labels_joined,  # 원문 라벨도 유지(원하면 제거 가능)\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "def click_next_page(driver):\n",
    "    candidates = [\n",
    "        (By.CSS_SELECTOR, 'a[rel=\"next\"]'),\n",
    "        (By.XPATH, \"//a[contains(normalize-space(.), '다음')]\"),\n",
    "        (By.CSS_SELECTOR, \".pagination a.next, .paginate a.next, a.page-link.next\"),\n",
    "    ]\n",
    "    for by, sel in candidates:\n",
    "        try:\n",
    "            btns = driver.find_elements(by, sel)\n",
    "            for b in btns:\n",
    "                if b.is_displayed() and b.is_enabled():\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", b)\n",
    "                    time.sleep(0.2)\n",
    "                    b.click()\n",
    "                    return True\n",
    "        except:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "def crawl_and_save(url=URL, headless=False, delay_between_pages=0.8, out_csv=\"bluer_gangnam\".csv\"):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--window-size=1400,2200\")\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        all_rows = []\n",
    "        page = 1\n",
    "        while True:\n",
    "            page_rows = get_cards_on_page(driver)\n",
    "            print(f\"[PAGE {page}] rows: {len(page_rows)}\")\n",
    "            all_rows.extend(page_rows)\n",
    "\n",
    "            if not click_next_page(driver):\n",
    "                break\n",
    "            page += 1\n",
    "            time.sleep(delay_between_pages)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    # 원하는 컬럼 순서로 저장\n",
    "    cols = [\"가게\", \"주소\", \"카테고리\", \"리본개수\", \"레드리본 선정\", \"서울 2025 선정\", \"라벨\"]\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    df = df[cols]\n",
    "    df = df.drop_duplicates(subset=[\"가게\", \"주소\"])\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"saved: {out_csv} ({len(df)} rows)\")\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_and_save(headless=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d7508e",
   "metadata": {},
   "source": [
    "## 페이지 넘어가는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "662b339d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] total pages: 130\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=141.0.7390.77)\nStacktrace:\n\tGetHandleVerifier [0x0x7ff7109be9e5+80021]\n\tGetHandleVerifier [0x0x7ff7109bea40+80112]\n\t(No symbol) [0x0x7ff71074060f]\n\t(No symbol) [0x0x7ff7107182f1]\n\t(No symbol) [0x0x7ff7107c88be]\n\t(No symbol) [0x0x7ff7107e8fa2]\n\t(No symbol) [0x0x7ff7107c1003]\n\t(No symbol) [0x0x7ff7107895d1]\n\t(No symbol) [0x0x7ff71078a3f3]\n\tGetHandleVerifier [0x0x7ff710c7dd8d+2960445]\n\tGetHandleVerifier [0x0x7ff710c7804a+2936570]\n\tGetHandleVerifier [0x0x7ff710c98a87+3070263]\n\tGetHandleVerifier [0x0x7ff7109d84ce+185214]\n\tGetHandleVerifier [0x0x7ff7109dff1f+216527]\n\tGetHandleVerifier [0x0x7ff7109c7c24+117460]\n\tGetHandleVerifier [0x0x7ff7109c7ddf+117903]\n\tGetHandleVerifier [0x0x7ff7109adcb8+11112]\n\tBaseThreadInitThunk [0x0x7ffeb9e5e8d7+23]\n\tRtlUserThreadStart [0x0x7ffebae28d9c+44]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 206\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 206\u001b[0m     crawl_all_pages_click(headless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[1], line 187\u001b[0m, in \u001b[0;36mcrawl_all_pages_click\u001b[1;34m(start_url, headless, out_csv)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m page \u001b[38;5;241m!=\u001b[39m cur:\n\u001b[0;32m    185\u001b[0m     go_to_page(driver, page)\n\u001b[1;32m--> 187\u001b[0m page_rows \u001b[38;5;241m=\u001b[39m extract_cards(driver)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAGE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(page_rows)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m page_rows:\n",
      "Cell \u001b[1;32mIn[1], line 129\u001b[0m, in \u001b[0;36mextract_cards\u001b[1;34m(driver)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# 보조 파싱\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (name \u001b[38;5;129;01mand\u001b[39;00m address):\n\u001b[1;32m--> 129\u001b[0m     html \u001b[38;5;241m=\u001b[39m card\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minnerHTML\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    130\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name:\n",
      "File \u001b[1;32mc:\\Users\\MYNOTE\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:231\u001b[0m, in \u001b[0;36mWebElement.get_attribute\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m getAttribute_js \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m     _load_js()\n\u001b[1;32m--> 231\u001b[0m attribute_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mexecute_script(\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/* getAttribute */return (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgetAttribute_js\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).apply(null, arguments);\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, name\n\u001b[0;32m    233\u001b[0m )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attribute_value\n",
      "File \u001b[1;32mc:\\Users\\MYNOTE\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:555\u001b[0m, in \u001b[0;36mWebDriver.execute_script\u001b[1;34m(self, script, *args)\u001b[0m\n\u001b[0;32m    552\u001b[0m converted_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(args)\n\u001b[0;32m    553\u001b[0m command \u001b[38;5;241m=\u001b[39m Command\u001b[38;5;241m.\u001b[39mW3C_EXECUTE_SCRIPT\n\u001b[1;32m--> 555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(command, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m\"\u001b[39m: script, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: converted_args})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\MYNOTE\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:458\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    455\u001b[0m response \u001b[38;5;241m=\u001b[39m cast(RemoteConnection, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor)\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    459\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\MYNOTE\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    230\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=141.0.7390.77)\nStacktrace:\n\tGetHandleVerifier [0x0x7ff7109be9e5+80021]\n\tGetHandleVerifier [0x0x7ff7109bea40+80112]\n\t(No symbol) [0x0x7ff71074060f]\n\t(No symbol) [0x0x7ff7107182f1]\n\t(No symbol) [0x0x7ff7107c88be]\n\t(No symbol) [0x0x7ff7107e8fa2]\n\t(No symbol) [0x0x7ff7107c1003]\n\t(No symbol) [0x0x7ff7107895d1]\n\t(No symbol) [0x0x7ff71078a3f3]\n\tGetHandleVerifier [0x0x7ff710c7dd8d+2960445]\n\tGetHandleVerifier [0x0x7ff710c7804a+2936570]\n\tGetHandleVerifier [0x0x7ff710c98a87+3070263]\n\tGetHandleVerifier [0x0x7ff7109d84ce+185214]\n\tGetHandleVerifier [0x0x7ff7109dff1f+216527]\n\tGetHandleVerifier [0x0x7ff7109c7c24+117460]\n\tGetHandleVerifier [0x0x7ff7109c7ddf+117903]\n\tGetHandleVerifier [0x0x7ff7109adcb8+11112]\n\tBaseThreadInitThunk [0x0x7ffeb9e5e8d7+23]\n\tRtlUserThreadStart [0x0x7ffebae28d9c+44]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "START_URL = \"https://www.bluer.co.kr/search?query=&zone1=%EC%84%9C%EC%9A%B8%20%EA%B0%95%EB%B6%81\"\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    return \" \".join(s.split()) if s else s\n",
    "\n",
    "def yesno(flag: bool) -> str:\n",
    "    return \"Y\" if flag else \"N\"\n",
    "\n",
    "def wait_cards(driver, timeout=15):\n",
    "    WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"li.rl-col.restaurant-thumb-item\"))\n",
    "    )\n",
    "    time.sleep(0.4)  # 렌더 안정화\n",
    "\n",
    "def get_total_pages(driver) -> int:\n",
    "    \"\"\"ul.pagination.bootpag에서 마지막 페이지(data-lp) 추출\"\"\"\n",
    "    try:\n",
    "        pager = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"ul.pagination.bootpag\"))\n",
    "        )\n",
    "    except:\n",
    "        return 1\n",
    "    # last 버튼 우선\n",
    "    last = pager.find_elements(By.CSS_SELECTOR, \"li.last[data-lp]\")\n",
    "    if last:\n",
    "        try:\n",
    "            return int(last[0].get_attribute(\"data-lp\"))\n",
    "        except:\n",
    "            pass\n",
    "    # 보조: 숫자 li 중 최대 data-lp\n",
    "    nums = pager.find_elements(By.CSS_SELECTOR, \"li[data-lp]\")\n",
    "    max_lp = 1\n",
    "    for li in nums:\n",
    "        try:\n",
    "            lp = int(li.get_attribute(\"data-lp\"))\n",
    "            if lp > max_lp:\n",
    "                max_lp = lp\n",
    "        except:\n",
    "            continue\n",
    "    return max_lp\n",
    "\n",
    "def go_to_page(driver, page: int, timeout=12) -> bool:\n",
    "    \"\"\"li[data-lp=page] 클릭 후 active 페이지/카드 갱신 대기\"\"\"\n",
    "    # 현재 첫 카드(있으면) 참조해서 staleness 체크\n",
    "    old_first = None\n",
    "    try:\n",
    "        old_first = driver.find_element(By.CSS_SELECTOR, \"li.rl-col.restaurant-thumb-item\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # 페이지네이션 보이도록 살짝 스크롤\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(0.2)\n",
    "\n",
    "    # 타겟 li[data-lp=\"page\"] > a 클릭\n",
    "    btn = WebDriverWait(driver, 5).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, f'ul.pagination.bootpag li[data-lp=\"{page}\"] a'))\n",
    "    )\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", btn)\n",
    "    time.sleep(0.1)\n",
    "    btn.click()\n",
    "\n",
    "    # 1) active 페이지가 page로 바뀔 때까지 대기\n",
    "    WebDriverWait(driver, timeout).until(\n",
    "        lambda d: (d.find_element(By.CSS_SELECTOR, \"ul.pagination.bootpag li.active\")\n",
    "                   .get_attribute(\"data-lp\") == str(page))\n",
    "    )\n",
    "    # 2) 카드가 새로고침(staleness) 혹은 다시 로드될 때까지 대기\n",
    "    if old_first:\n",
    "        try:\n",
    "            WebDriverWait(driver, 6).until(EC.staleness_of(old_first))\n",
    "        except:\n",
    "            pass\n",
    "    wait_cards(driver, timeout=timeout)\n",
    "    return True\n",
    "\n",
    "def extract_cards(driver):\n",
    "    \"\"\"현재 페이지 카드 → dict 리스트\"\"\"\n",
    "    rows = []\n",
    "    cards = driver.find_elements(By.CSS_SELECTOR, \"li.rl-col.restaurant-thumb-item\")\n",
    "    for card in cards:\n",
    "        name = address = None\n",
    "        food_types_joined = None\n",
    "        ribbon_count = 0\n",
    "        labels_joined = None\n",
    "        has_red = False\n",
    "        has_seoul2025 = False\n",
    "\n",
    "        # 가게명\n",
    "        try:\n",
    "            h3 = card.find_element(By.CSS_SELECTOR, \".header-title h3\")\n",
    "            name = clean_text(h3.text) or clean_text(h3.get_attribute(\"innerText\"))\n",
    "        except: pass\n",
    "        # 주소\n",
    "        try:\n",
    "            addr_el = card.find_element(By.CSS_SELECTOR, \".thumb-caption .info .info-item .content-info.juso-info\")\n",
    "            address = clean_text(addr_el.text) or clean_text(addr_el.get_attribute(\"innerText\"))\n",
    "        except: pass\n",
    "        # 카테고리(여러 개)\n",
    "        try:\n",
    "            ft_els = card.find_elements(By.CSS_SELECTOR, \".header-status .foodtype li\")\n",
    "            food_types = [clean_text(el.text) for el in ft_els if clean_text(el.text)]\n",
    "            food_types_joined = \", \".join(food_types) if food_types else None\n",
    "        except: pass\n",
    "        # 리본 개수(이미지 개수)\n",
    "        try:\n",
    "            ribbon_count = len(card.find_elements(By.CSS_SELECTOR, \".header-title .ribbons .img-ribbon\"))\n",
    "        except: ribbon_count = 0\n",
    "        # 라벨 + 플래그\n",
    "        try:\n",
    "            label_els = card.find_elements(By.CSS_SELECTOR, \".header-title .header-labels li\")\n",
    "            labels = [clean_text(el.text) for el in label_els if clean_text(el.text)]\n",
    "            labels_joined = \" \".join(labels) if labels else None\n",
    "            has_red = any(\"레드리본 선정\" in t for t in labels)\n",
    "            has_seoul2025 = any(\"서울 2025 선정\" in t for t in labels)\n",
    "        except: pass\n",
    "\n",
    "        # 보조 파싱\n",
    "        if not (name and address):\n",
    "            html = card.get_attribute(\"innerHTML\")\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            if not name:\n",
    "                el = soup.select_one(\".header-title h3, .clearfix > h3\")\n",
    "                name = clean_text(el.get_text()) if el else name\n",
    "            if not address:\n",
    "                el = soup.select_one(\".content-info.juso-info\")\n",
    "                address = clean_text(el.get_text()) if el else address\n",
    "            if food_types_joined is None:\n",
    "                fts = [clean_text(li.get_text()) for li in soup.select(\".header-status .foodtype li\") if clean_text(li.get_text())]\n",
    "                food_types_joined = \", \".join(fts) if fts else None\n",
    "            if ribbon_count == 0:\n",
    "                ribbon_count = len(soup.select(\".header-title .ribbons .img-ribbon\"))\n",
    "            if labels_joined is None:\n",
    "                labs = [clean_text(li.get_text()) for li in soup.select(\".header-title .header-labels li\")]\n",
    "                labels_joined = \" \".join(labs) if labs else None\n",
    "                has_red = any(\"레드리본 선정\" in t for t in labs)\n",
    "                has_seoul2025 = any(\"서울 2025 선정\" in t for t in labs)\n",
    "\n",
    "        if name or address:\n",
    "            rows.append({\n",
    "                \"가게\": name,\n",
    "                \"주소\": address,\n",
    "                \"카테고리\": food_types_joined,\n",
    "                \"리본개수\": ribbon_count,\n",
    "                \"레드리본 선정\": yesno(has_red),\n",
    "                \"서울 2025 선정\": yesno(has_seoul2025),\n",
    "                \"라벨\": labels_joined,\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "def crawl_all_pages_click(start_url=START_URL, headless=False, out_csv=\"bluer_gangnam.csv\"):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--window-size=1400,2200\")\n",
    "    # (필요 시 탐지 회피 옵션 추가 가능)\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "    all_rows, seen = [], set()\n",
    "    try:\n",
    "        driver.get(start_url)\n",
    "        wait_cards(driver)\n",
    "\n",
    "        total = get_total_pages(driver)\n",
    "        print(f\"[INFO] total pages: {total}\")\n",
    "\n",
    "        # 현재 활성 페이지\n",
    "        try:\n",
    "            cur = int(driver.find_element(By.CSS_SELECTOR, \"ul.pagination.bootpag li.active\").get_attribute(\"data-lp\"))\n",
    "        except:\n",
    "            cur = 1\n",
    "\n",
    "        # cur ~ total까지 순회\n",
    "        for page in range(cur, total + 1):\n",
    "            if page != cur:\n",
    "                go_to_page(driver, page)\n",
    "\n",
    "            page_rows = extract_cards(driver)\n",
    "            print(f\"[PAGE {page}] rows: {len(page_rows)}\")\n",
    "\n",
    "            for r in page_rows:\n",
    "                key = (r[\"가게\"], r[\"주소\"])\n",
    "                if key in seen:\n",
    "                    continue\n",
    "                seen.add(key)\n",
    "                all_rows.append(r)\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    df = pd.DataFrame(all_rows, columns=[\"가게\", \"주소\", \"카테고리\", \"리본개수\", \"레드리본 선정\", \"서울 2025 선정\", \"라벨\"])\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"saved: {out_csv} ({len(df)} rows)\")\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_all_pages_click(headless=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624d40ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 원본이 utf-8-sig로 저장되었다고 가정 (다르면 encoding 바꿔보세요: 'utf-8', 'cp949' 등)\n",
    "df = pd.read_csv(\"blue_food.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# 엑셀 호환 좋게 cp949로 다시 저장\n",
    "df.to_csv(\"bluer_food_cp949.csv\", index=False, encoding=\"cp949\")\n",
    "\n",
    "# 또는 UTF-8 with BOM으로 다시 저장\n",
    "df.to_csv(\"bluer_food_utf8sig.csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
