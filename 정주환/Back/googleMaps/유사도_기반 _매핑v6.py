import pandas as pd
from rapidfuzz import process, fuzz
import sys
import re

# tqdm ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì •
try:
    from tqdm import tqdm
    tqdm.pandas(desc="4ë‹¨ê³„ ì •ë°€ ë§¤í•‘ ì¤‘")
    USE_TQDM = True
except ImportError:
    USE_TQDM = False

# --- ì‚¬ìš©ì ì„¤ì • ---
REVIEW_FILE = 'all_reviews_processed.csv'
BLUERIBBON_FILE = '20251016_ì„œìš¸ì‹œ_ìŒì‹ì _ëª©ë¡_GPS.csv'
REVIEW_ID_COL = 'place_id'
REVIEW_NAME_COL = 'place_name'
BLUERIBBON_NAME_COL = 'ê°€ê²Œ'
BLUERIBBON_ID_COL = 'id'

SCORE_HIGH = 90  # ì•ˆì „ êµ¬ê°„
SCORE_LOW = 75   # í™•ì¸í•„ìš” êµ¬ê°„

OUTPUT_FILE_ALL = 'final_result_v17_all.csv'
OUTPUT_FILE_VERIFY = 'matches_needing_verification.csv' # í™•ì¸í•„ìš” ëª©ë¡ ë³„ë„ ì €ì¥
# -------------------

def clean_std(name): return re.sub(r'[^ê°€-í£a-zA-Z0-9]', ' ', str(name)).strip().lower() if pd.notna(name) else ""
def clean_hangul(name): return re.sub(r'[^ê°€-í£]', '', str(name)) if pd.notna(name) else ""
def clean_eng(name): return re.sub(r'[^a-zA-Z]', '', str(name)).lower() if pd.notna(name) else ""

print("75ì  ë¡œê·¸ ê¸°ëŠ¥ì´ í¬í•¨ëœ ë§¤í•‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

# 1. ë°ì´í„° ë¡œë“œ
try:
    df_reviews = pd.read_csv(REVIEW_FILE)
    df_blueribbon = pd.read_csv(BLUERIBBON_FILE)
except Exception as e:
    print(f"[ì˜¤ë¥˜] {e}")
    sys.exit()

# 2. ì°¸ì¡° ë°ì´í„° ì¤€ë¹„
df_blueribbon['c_std'] = df_blueribbon[BLUERIBBON_NAME_COL].apply(clean_std)
df_blueribbon['c_hangul'] = df_blueribbon[BLUERIBBON_NAME_COL].apply(clean_hangul)
df_blueribbon['c_eng'] = df_blueribbon[BLUERIBBON_NAME_COL].apply(clean_eng)

ref_std = df_blueribbon.set_index('c_std')[BLUERIBBON_NAME_COL].to_dict()
ref_hangul = df_blueribbon.set_index('c_hangul')[BLUERIBBON_NAME_COL].to_dict()
ref_eng = df_blueribbon.set_index('c_eng')[BLUERIBBON_NAME_COL].to_dict()
for ref in [ref_std, ref_hangul, ref_eng]: ref.pop('', None)

list_std = list(ref_std.keys())
list_hangul = list(ref_hangul.keys())
list_eng = list(ref_eng.keys())

# 3. 4ë‹¨ê³„ ë§¤í•‘ í•¨ìˆ˜
def find_match_v17(original_name):
    # [1ë‹¨ê³„] í‘œì¤€ (90ì )
    t_std = clean_std(original_name)
    if t_std:
        m = process.extractOne(t_std, list_std, scorer=fuzz.token_set_ratio, score_cutoff=SCORE_HIGH)
        if m: return ref_std[m[0]], m[1], "1ì°¨(ì•ˆì „)"

    # [2ë‹¨ê³„] í•œê¸€ (90ì )
    t_hangul = clean_hangul(original_name)
    if len(t_hangul) >= 2:
        m = process.extractOne(t_hangul, list_hangul, scorer=fuzz.WRatio, score_cutoff=SCORE_HIGH)
        if m: return ref_hangul[m[0]], m[1], "2ì°¨(í•œê¸€ì•ˆì „)"

    # [3ë‹¨ê³„] ì˜ì–´ (90ì )
    t_eng = clean_eng(original_name)
    if len(t_eng) >= 3:
        m = process.extractOne(t_eng, list_eng, scorer=fuzz.WRatio, score_cutoff=SCORE_HIGH)
        if m: return ref_eng[m[0]], m[1], "3ì°¨(ì˜ì–´ì•ˆì „)"

    # [4ë‹¨ê³„] â˜… ìµœí›„í†µì²© (75ì ) - ë¡œê·¸ ëŒ€ìƒ â˜…
    if t_std:
        m = process.extractOne(t_std, list_std, scorer=fuzz.WRatio, score_cutoff=SCORE_LOW)
        if m: return ref_std[m[0]], m[1], "4ì°¨(í™•ì¸í•„ìš”)"

    return None, 0, "ì‹¤íŒ¨"

# 4. ì‹¤í–‰
unique_shops = df_reviews.groupby(REVIEW_ID_COL)[REVIEW_NAME_COL].first().reset_index()
if USE_TQDM:
    results = unique_shops[REVIEW_NAME_COL].progress_apply(find_match_v17)
else:
    results = unique_shops[REVIEW_NAME_COL].apply(find_match_v17)

unique_shops[['matched_name', 'match_score', 'match_method']] = pd.DataFrame(results.tolist(), index=unique_shops.index)

# 5. ID ë§¤í•‘ ë° ì €ì¥
name_to_id = df_blueribbon.set_index(BLUERIBBON_NAME_COL)[BLUERIBBON_ID_COL].to_dict()
unique_shops['matched_id'] = unique_shops['matched_name'].map(name_to_id)

df_merged = pd.merge(df_reviews, unique_shops, on=[REVIEW_ID_COL, REVIEW_NAME_COL], how='left')
final_df = pd.merge(df_merged, df_blueribbon.drop(columns=['c_std', 'c_hangul', 'c_eng']), left_on='matched_id', right_on=BLUERIBBON_ID_COL, how='left', suffixes=('', '_br'))

final_df.to_csv(OUTPUT_FILE_ALL, index=False, encoding='utf-8-sig')

# 6. â˜… í™•ì¸í•„ìš” ë¡œê·¸ ì¶œë ¥ ë° ë³„ë„ ì €ì¥ â˜…
verify_list = unique_shops[unique_shops['match_method'] == "4ì°¨(í™•ì¸í•„ìš”)"]
verify_list.to_csv(OUTPUT_FILE_VERIFY, index=False, encoding='utf-8-sig')

print(f"\n========== [v17 ë§¤í•‘ ê²°ê³¼ ë¦¬í¬íŠ¸] ==========")
print(f"ì „ì²´ ì„±ê³µ: {unique_shops['matched_id'].notna().sum()} / {len(unique_shops)} ({(unique_shops['matched_id'].notna().sum()/len(unique_shops))*100:.2f}%)")
print(f"ë‹¨ê³„ë³„ ì„±ê³µ ìˆ˜:\n{unique_shops['match_method'].value_counts()}")
print(f"--------------------------------------------")
print(f"âš ï¸ [í™•ì¸í•„ìš”] 75~89ì  êµ¬ê°„ ë§¤í•‘: {len(verify_list)}ê±´")
print(f"ëª©ë¡ì´ '{OUTPUT_FILE_VERIFY}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print(f"--------------------------------------------")
print("ğŸ” [í™•ì¸í•„ìš”] ìƒ˜í”Œ ë¡œê·¸ (ìƒìœ„ 20ê°œ):")
for _, row in verify_list.head(20).iterrows():
    print(f" - [ë¦¬ë·°] {row[REVIEW_NAME_COL]}  <--({row['match_score']:.1f}ì )-->  [ë¸”ë£¨ë¦¬ë³¸] {row['matched_name']}")
print("============================================")